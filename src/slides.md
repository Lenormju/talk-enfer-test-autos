# L'enfer des tests autos

Notes:
* TODO :
  * exemples concrets tout du long
  * exemples de code/archi √† la fin
  * images
  * FIXME: the slides should be vertical (cf mkslides_config.yml)

---

# 0. Introduction

Notes:
* TODO :
  * illustration de chapitre ?

-v-

## Disclaimer on n'est pas parfaits

Notes:
* on n'est pas parfait, des fois on ne teste pas (assez), ou pas auto

-v-

## Sondage : qui trouve que c'est l'enfer

Notes:
* qui trouve que les tests c'est l'enfer ? Qui trouve que c'est le paradis ? le purgatoire ?

-v-

## Pr√©sentation

Julien Lenormand üòá

Jonathan Gaffiot üëπ
@ Kaizen Solutions

Notes:
* TODO :
  * garder cette section ici ?

-v-

## Tester ?

Notes:
* c'est quoi tester ? c'est quoi tester automatiquement ? (moment chiant avec des d√©finitions)
* action, r√©action, stimuli, SUT
  * d√©finiton test = s'assurer de la r√©ponse attendue de la part du syst√®me dans un certain √©tat √† un stimuli particulier
* Qualit√© avec un grand Q : (d'apr√®s Rambo Python) fiabilit√©, maintenabilit√©, √©volutivit√©, s√©curit√©
    * mentionner ISO-truc pour une autre d√©finition (plus large)

---

# 1. Pourquoi c'est important les tests autos ? (il faut le rappeller !)

Notes:
* TODO :
  * illustration de chapitre ?
  * @Julien refaire une passe sur les r√©p√©titions (cf r√©union du mercredi 08 octobre)
  * scientific proofs of efficiency ?

-v-

## Confiance

Notes:
* s√©r√©nit√©
* mise en prod le vendredi apr√®s-midi
* confiance dans ce qui a chang√©, confiance dans ce qui n'a pas chang√©

-v-

## Feedback rapide

Notes:
* ownership de la qualit√© du code, ce n'est pas juste aux QAs, ou utilisateurs de trouver les bugs, "√ßa marche sur ma machine"
* la qualit√© c'est une d√©marche, un tamis, un empilement (vrai sens de Kanban), une culture (LEAN, Kanban "right the first time")
* facile √† ex√©cuter : un clic et c'est bon, √ßa part en prod

-v-

## Qualit√© (fiabilit√©) du code

Notes:
* et les autres ? maintenabilit√©/√©volutivit√©/s√©curit√© !
* s'autoriser le refactoring, conserver du code maintenable
* stabilit√© / perennit√© / scalabilit√© humaine-temporelle-technique-complexit√©-busfactor
  * Tests auto = moyen de scaler dans le temps, la taille, les personnes, les techs, ...
* quelle valeur √† un logiciel qui ne peut pas √™tre test√© automatiquement ? uniquement court-terme
* pas d'autom == risque projet
* une certaine forme de sp√©cification (c'est plus simple de savoir ce que le code doit faire, quand c'est litt√©ralement commit√© dans le repo)
    * et encore mieux, elle se v√©rifie toute seule !!
* ex√©cution automatique/syst√©matique -> pas besoin de devoir s'en souvenir (CI, make, ...)
* emp√™che le "Fear driven development"
  * Comment faire du fearless refactoring sans Rust ni test ?
* √©vite le "tomb√© en marche"
* non-reg
* Sens strict de refactoring, pas de refactoring sans garantie que le comportement "observable" (externe) n'a pas √©volu√©
  * n√©cessaire pour dompter la dette technique

-v-

## √âthique professionnelle

Notes:
* (cf Craft), d√©ontologie
* responsabilit√© en tant que dev, que Crafteur, que prestataire; que ...
* crit√®re de validit√© de ce qui est livr√© ("si c'est pas test√©, c'est r√©put√© ne pas marcher")
* pas oblig√© de faire comme les autres
* livraison, recette
* maturit√© professionnelle

-v-

## Rentabilit√©

Notes:
* Accelerate
* TODO autres preuves d'efficacit√© ? (cf scientific proofs)
* se concentrer sur des activit√©s √† forte valeur ajout√©e, par rapport √† r√©p√©ter des tests
* Seul moyen de tenir la cadence

-v-

## Le paradis !

Notes:
* une fois qu'on s'est dit √ßa, √ßa para√Æt vachement bien, donc y'a aucune raison de pas en faire
* meme avec l'image recto/verso, ville en feu, b√©b√© zombie
  * "Kid Thrown In The Air Meme: How Dad Sees It Vs How Mom Sees It" cf https://i.imgur.com/qL915f0.jpeg
* Stop au masochisme ! (TODO: autre section ?)
* transition vers la partie suivante

---

# 2. Pourquoi c'est difficile les tests autos ? (il faut bien l'avouer !)

Notes:
* TODO: le formuler de fa√ßon √† r√©pondre au chemin de cr√™te ?
* TODO: ajouter des exemples concrets √† chacun

-v-

## Chemin de cr√™te

Notes:
* m√©taphore du chemin de cr√™te : facile de tomber

-v-

## Pas appris

Notes:
* sauf pour les testeurs de m√©tier, les moldus s'en passeront bien ?
* pas de formation dans la plupart des cursus master, ou bien th√©orique ou tr√®s court
* assez peu pr√©sent dans la litt√©rature g√©n√©raliste, malgr√© sa pr√©valence et importance (cf biblio)
* pas un sujet "sexy" (formation continue, conf√©rences, ...)
* exemple Ensimag, mon cursus versus ce qui est propos√© actuellement
    * Mon √®cole d'ing√©, ni mon DUT ne m'ont enseign√© le test
    * Le test √®tait une option, une ann√©e, d'une fili√®re
    * les cours de Groz sur la v√©rif statique et la mod√©lisation boite noire
    * Ensimag, examens de SAP en 2014 et 2015 :
      * d√©terminer les (in)variants de boucle
      * preuve d'arr√™t,
      * interpr√®tation abstraite (r√©duction de machines √† √©tats),
      * acc√®s m√©moire invalide,
      * valeurs par d√©faut en Java,
      * d√©tection de la r√©cursion ou de code mort,
      * propagation d'intervalles pour ex√©cution symbolique
      * preuves de comportement de programme
    * https://ensimag.grenoble-inp.fr/fr/formation/analyse-de-code-pour-la-s-ucirc-ret-eacute-et-la-s-eacute-curit-eacute-4mmacss
      * Ce cours est une introduction aux fondements de la s√©mantique et l‚Äôanalyse de programmes. Il offre les bases sur lesquelles s‚Äôappuyer pour sp√©cifier et d√©velopper des applications s√ªres, construire et se servir d‚Äôoutils d‚Äôanalyse et de v√©rification.
      * S√©mantique op√©rationnelle des langages de programmation.
      * Calcul de plus faible pr√©condition et preuve de programmes.
      * Analyse de flot de donn√©es.
      * Analyse statique et interpr√©tation abstraite.
      * Applications √† la compilation, √† la s√ªret√© et √† la s√©curit√© des logiciels.
      * Travaux pratiques √† l'aide de 2 outils industriels.
        * (Frama-C et Coq s√ªrement ?)
    * https://ensimag.grenoble-inp.fr/fr/formation/test-des-syst-egrave-mes-logiciels-5mmtsl6
      * une option parmi 3, pour ceux qui font pas d'Erasmus
      * Pr√©sentation des m√©thodes de test pour assurer la s√ªret√© de fonctionnement des logiciels.
      * Test
      * V√©rification et validation
      * Les tests au cours du cycle de vie.
      * Test structurel des logiciels.
      * Test √† partir des sp√©cifications: partitionnement, combinatoire.
      * M√©thodes de test bas√©es sur des mod√®les, en particulier automates.
      * Analyse des notions de couverture, test mutationnel.
      * Eclairage sur des domaines de test importants:
        * test de performance et test de charge
        * test d'interface
        * test de s√©curit√©.
      * Cours de g√©nie logiciel abordant notamment les cycles de d√©veloppement : cela permet de situer correctement le test dans une activit√© de d√©veloppement.  
      * Bonnes connaissances en algorithmique et programmation : √™tre capable d'analyser un programme, de l'ex√©cuter symboliquement "√† la main", fait partie des activit√©s du testeur et est une comp√©tence indispensable pour comprendre les techniques fond√©es sur l'analyse du code.  
      * Langages et automates : une partie du cours porte sur de mod√®les et en particulier des machines d'√©tats finis exploit√©es pour engendrer des tests de conformit√©.
      * Bibliographie :
        * Aditya P. Mathur:Foundations of Spftware Testing, Pearson 2008.  
        * J-F. Pradat-Peyre, J. Printz: Pratique des tests logiciels, Dunod 2009.  
        * Myers, G.J. : The Art of Software Testing. Wiley 1979; r√©√©dit√© 2004.
    * le cours de derni√®re ann√©e
      * @Julien TODO zip de quentin pign√©
* "missing semester" ?
* Apprentissage th√©orique (en √©tudes ing√©) versus apprentissage empirique de l'informatique (sur le terrain), en particulier du test

-v-

## Vocabulaire confusant

Notes:
* personne n'est d'accord sur rien : 47 pyramides diff√©rentes, le vocabulaire du test,~~les perspectives tech~~, les r√¥les, les niveaux de test
* lister et illustrer avec diff√©rents types de pyramides
    * blague illuminatis (pyramide)
    * On est des √®gyptiens, on a plein plein de pyramides diff√®rentes
* Pyramide originale par Mike Cohn dans "Succeeding with Agile" publi√© en 2009
* Floril√®ge des autres formes : ice cream, hourglass, diamond, upside-down pyramid, trophy (kent beck) ... --> aucune solution universelle
    * https://claude.ai/chat/3ff66008-4cc7-431e-9657-9f4987e7d86c
* Dimensions de la pyramide : vitesse d'ex√©cution, co√ªt √† √©crire/maintenir, quantit√© de code exerc√©e, fid√©lit√© utilisateur, ...
  * Multiples interpr√©tations des dimensions de la pyramide : quantit√©, vitesse d'ex√©cution, confiance, sp√©cificit√©, couverture apport√©e, co√ªt de production, fr√©quence de changement / bug, ...
* Le test c'est un sujet transverse our chaque dev, qu'importe le langage, la stack, le m√©tier, ... Donc tout le monde parle de quelque chose avec un point de vue et un focus diff√®rent, pas toujours mentionn√©
* pr√™t-√†-penser
    * dans quels contextes-projets travaillent les diff√©rentes personnes qui ont propos√© ces pyramides ? et en quelle ann√©e ?
    * ne suffit pas pour appr√©hender le test
    * Testing tools have evolved in 30 years
* jeu des 7314 diff√©rences : industrie, technos, maturit√© du produit, dur√©e de maintenance, culture d'√©quipe, comp√©tences lacunaires, vitesse, confiance, ...
    * il n'y en a pas qu'une seule, mais une par projet ! chaque projet est diff√©rent !
    * chaque projet est (quasi) unique
* anecdote sur le vocabulaire pas unifi√© : du temps de mon stage en autom de test, j'avais travaill√© entre autres sur un glossaire unifi√© entre ISTQB et CFTL
* meilleure d√©finition des axes : "finalit√©, granularit√©, modalit√© d'assertion" (50 shades ...)
* essayons de poser une d√©finition, dans notre contexte, des mots que nous employons
    * diff√©rences entre ISTQB et CFTL, cf mon stage Sogeti
* tous les diff√©rents types de test : carac, fonc, integ, unit, acceptance, user-testing, accessibility, composant-unitaire versus composant-UI, e2e-stack ou e2e-sc√©nario...
    * TODO more
    * charge : soak, breakpoint, ... (cf doc de k6 avec de jolis graphiques)
        * K6 typologie : breakpoint, soak, stress, load, ...
    * cf ISO 15010
    * peu de personnes, m√™me dans des livres (exemple Clean Code de Robert C Martin "Uncle Bob"), ne font l'effort de d√©finir
    * Pyramyde invers√© de la valeur : le haut niveau est le plus pertinent, mais cher et fragile ?
        * Facilitation des tests web d'ui (selenium, cypress, playwright)
* QA analyst/tester vs quality engineer vs testeur-automaticien vs "dev" vs IVVQ vs QA "quality advisor" vs test manager ...
* Test error vs test failure : le test n'a pas abouti pour une raison technique (probl√®me de test), le test a abouti mais n'a pas produit le r√©sultat attendu (probl√®me fonctionnel on esp√®re !)
  * distinction "test qui plante" versus "test qui √©choue"
* Imbrication et interconnexion des syst√®mes : les tests au m√™me endroit n'ont pas le m√™me nom / fonction.
* politique vs strat√©gie vs plan de test
* E2e : vertical de la stack ? Ou horizontal du parcours utilisateur ?
* Sanity vs smoke ?
* test fonctionnel + non-fonctionnel : si la perf fait partie des requirements, alors fonctionnel ou pas ?
* [le glossaire ISTQB](https://glossary.istqb.org/en_US/search?term=) donne 601 r√©sultats en anglais, 559 en fran√ßais

-v-

## Trop de tests

Notes:
* explosion combinatoire
* cible mouvante et floue
* L'explosion combinatoire rend l'exhaustivit√© impossible

-v-

## Pratique r√©ticente

Notes:
* des tonnes d'outils diff√©rents, les diff√©rents types de tests √©voqu√©s, les diff√©rents m√©tiers, l'insertion dans le process de production, ...
* s'organiser, planifier et r√©aliser sont des t√¢ches complexes

-v-

## Punition

Notes:
* l'absence de testeur/expertise/culture dans les projets (des gens form√©s, motiv√©s, avec le mindset ad√©quat)
* > On peut conduire un cheval √† l'abreuvoir, mais pas le forcer √† boire
* certifications ISTQB par le CFTL
* casse-pied de service

-v-

## Inutile

Notes:
* convaincre (le management et/ou les devs) que c'est utile, avant de se manger une mise-en-prod foir√©e
* d√©pense versus √©conomie
* R√©sultats intangibles

-v-

## Test impossible

Notes:
* imitations techniques, mat√©rielles, de co√ªt, ... variabilit√© selon les environnements, pas de donn√©es de test (r√©alistes)
* exemple board farm Schneider
* exemple Windows 10 LTSC 2019 √† Thales

-v-

## Code intestable

Notes:
* Ennemis : side-effects ("spooky action at a distance", "que fait cette m√©thode ?"), (global/static) state, IO, singletons, time, locale, network, files, env, GPU, unclear pre/post-conditions, non-determinism, (G)UI vs API, concurrency et threading, random (non-deterministic), complex outputs and high dimensionality
* diff√©rence entre "c'est compliqu√© de r√©aliser le test" (limitations tech) versus "c'est compliqu√© d'√©crire le test" (iceberg, gorille)
* exemple : code des bornes qui √©choue le 29 F√©vrier
* exemple : code du Edge qui est correct sur la timezone 6 mois par an
* anecdote schneider :
  * on me demande de rajouter un param√®tre bool√©en √† une route HTTP, sur un composant sur lequel je n'ai jamais travaill√©
  * je me dis je vais le faire en TDD, commencer par √©crire un test
  * mais y'a aucun test sur le projet, donc je dois commencer par √©crire un test avant
  * j'√©cris un test, je le lance un fois il passe, deux fois il √©choue
  * car il avait une d√©pendance sur une base de donn√©es, laquelle est g√©r√©e par un singleton, qui stocke l'√©tat
  * donc je fixturise le singleton pour le r√©initialiser (ou le d√©truire/reconstruire) afin que mon test soit r√©p√©table
  * ensuite j'ajoute mon test et le changement est easy
  * (ah, maintenant faut que j'ajoute une CI !!)

-v-

## Fragile

Notes:
* couplage versus maintenabilit√© en carton, tests cass√©s pas r√©par√©s ("vitre cass√©e"), maintenance des tests v√©cue comme un fardeau
  * le summum : test de mock !
* Tester le mauvais 80% : m√©taphore du streetlight problem ("plus simple de chercher dans la lumi√®re")
* test flaky
  * l'√©quivalent de "pt√®t bin qu'oui, pt√®t bin qu'non"
  * suffit de les relancer plusieurs fois mdr, ~z√©ro confiance

-v-

## Illisible

Notes:
* exemple d'Eric (Schneider)

-v-

## Lent

Notes:
* lenteur, car tests lents et/ou trop de tests
* exemple de Xavier Nopre (cf [post LinkedIn](https://www.linkedin.com/posts/xnopre_pourquoi-jai-mis-plus-de-3-jours-%C3%A0-trouver-activity-7316027544934191104-Otww)), je lui ai dit qu'il y avait un probl√®me, c'est pas cens√© √™tre aussi lent
* exemple de Schneider : board farms
* exemple de Schneider : code dont le run dure des centaines jours !! (√† travers les timezones :p)

-v-

## Bug ou feature ou code mort ?

Notes:
* descriptivism vs prescriptivism (cf Romeu)
* test de caract√©risation OK, mais est-ce que c'est ce que √ßa devrait vraiment faire ? ü§∑
  * bug ou feature ?
  * xkcd workflow https://xkcd.com/1172/
* source de v√©rit√© = code ou spec Word ?

-v-

## L'enfer !

Notes:
* difficile de savoir quoi faire, comment faire, de le faire, √† ex√©cuter, √† analyser, √† maintenir, √† faire confiance, pas assez, pas assez bien, pas assez rapide, ...
* Facile √† dire de faire "le bon test", mais concr√®tement ?
  * y'a le bon testeur et le mauvais testeur ...
* transition

---

# 3. Comment faire pour bien tester auto ? (il faut s'aider !)

Notes:
* TODO:
  * orient√© solution cf partie pr√©c√©dente orient√© probl√®me ?
  * le formuler de fa√ßon √† r√©pondre au chemin de cr√™te
  * mentionner "se rendre dispensable" ? (vis-√†-vis du scaling humain)

-v-

## Chemin de cr√™te

Notes:
* comment le naviguer ? comment √™tre/devenir/rester rigoureux ?
  * discipline d'un moine bouddhiste, ou d'un garde de la reine d'Angleterre

-v-

## Culture de la qualit√©

Notes:
* culture qualit√©, formation (cours, conf√©rences, livres, exercices, katas, ...)
* cf nos recos √† la fin, exp√©rience (empirisme)
* compr√©hension du business et des stakeholders,
* tournure d'esprit (cf joke "un testeur rentre dans un bar, il commande -1 bi√®re, NaN bi√®re, demande o√π sont les toilettes ..."), "vicieux" pour "casser le code" et non pas seulement montrer qu'il fonctionne 
* exemple : SQLite testing, test code ratio, test harnesses, ... (+√©volution dans le temps)
* le code il faut le malmener
* la qualit√© c'est un ensemble de tamis successifs : empilement de couches pour attraper les bugs, du besoin, design, archi, implem, test, validation, d√©ploiement
  * processus/d√©marche au niveau deD√©finition de test unitaire contre-intuitive : ne pas penser microscopique/indivisible, mais coh√©rence/s√©paration-fronti√®re l'√©quipe/projet/entreprise/...
* exemple : NDP Systems
* "Le test n'apporte pas de valeur (argent) par rapport aux fonctionnalit√©s"
* Humilit√© de devoir tester
* "move fast and break things"
* https://news.ycombinator.com/item?id=13130991 : you get paid for "software", not "maintainable software"
* Avoir un "Patrick" (ou whatever name) dans son √©quipe, √©ternellement vigilant, le "relou"
* comme beaucoup de sujets, c'est pas un casse-pied de service qu'il faut, mais un changement de culture (beaucoup plus compliqu√©, cf Agile bullshit, s√©curit√©, ...)
* Responsabilit√© individuelle et d'√©quipe
* Lean, faire bien du premier coup, kanban, "right the first time"

-v-

## Strat√©gie de test

Notes:
* quoi pourquoi pour quoi comment qui quand ...
* sp√©cifications, Exigences et Requirements, business (value stream) et risques business, quadrants, matrice confiance versus risque, moyens (et toute la suite)
  * cartographier les flux d‚Äôinformation, les cas d‚Äôusage, les fronti√®res techniques et les domaines de l‚Äôapplication afin de discerner les fronti√®res des tests
* construisez votre "pyramide" (demander √† une IA diK6 typologie : breakpoint, soak, stress, load, ...ff√©rente pyramides, de plein de formes diff√©rentes √† la Dali), en se basant sur les moyens de test
* √† adapter/repenser r√©guli√®rement, comme tout le reste
* pourquoi, pour quoi, quoi, o√π, qui, quand et comment, ...
* trust boundaries (d√©pendences externes, parties peu fiables, risque m√©tier, responsabilit√©, vitesse d'√©volution, ...)
* Repenser la strat fr√©quemment, tout comme on le fait pour l'architecture de la solution
* consid√©rer les niveaux de test : Fonctionnalit√©s techniques (endpoint) versus user
* Quadrant des tests !! Axes : business vs tech, pour le produit vs √®quipe
* d√©pendences externes (on ne les controle pas, c'est compliqu√©, et elles peuvent changer sans qu'on y pr√™te attention) versus d√©pendances internes (on les controle, mais c'est quand m√™me compliqu√©, et peuvent changer si on ne fait pas assez gaffe)
* Choisir l'effort de test par p√©rim√®tre = choisir l√† o√π on pr√©f√®re diminuer la proba d'un bug
* contrats (boundaries)
* Plan de test au format IEEE 29119-3 (ou pas !)
* Le syst√®me test√© peut faire partie d'un sur-syst√®me, et se composer de sous-syst√®mes, les composants des uns sont les acceptation des autres
  * Ce qui d√©pend de nous versus ce qui ne d√®pend pas (sto√Øcisme)

-v-

## Moyens de test

Notes:
* humains, techniques, temporels, ...
* outils de test, formations, avoir le temps, d√©ployabilit√©, disponibilit√© du hardware, dispo des data, ...
* avoir des specs ! (claires)
* pas de bras, pas de chocolat

-v-

## Renoncer

Notes:
* renoncer √† tout automatiser (quadrants, moyens insuffisants, ...), ROI
* tradeof : cout, risque, complexit√©, ...
* crit√®re qui favorisent l'automatisation : r√©p√©tition, confiance dans l'autom, p√©nibilit√©, longueur, criticit√©, ...
* il vaut mieux un mix d'autom et de manuel, la force de chacun
* Le co√ªt des tests est parfois sup√©rieur aux ben√©fices
* exemple : lecteur de fichier √† EDF
* Tester le code techniquement complexe, sensible pour le m√©tier, et utile
* loi de pareto 80/20 : toujours fausse, toujours vraie
* hybrid possible aussi (soit manuel assist√© par autom, soit autom avec verif manuelle) : continuum Automatis√©-automatisable-manuel
* Sans jugement : un pas apr√®s l'autre, on h√©rite de codebases, on essaye de faire mieux
  * incr√©mental
* tester ne prouve pas l'absence de bugs, mais en √©limine certains  
* process avec des rendements d√©croissants, trouver le bon curseur, le bon √©quilibre
* garder des tests qu'on appr√©cie : rapides (ou moins rapides en CI mais + couvrants), fiables, maintenables, estimer le ROI
* OK de supprimer un test inutile
* fatalisme, tout tester en auto est un voeu pieux
* Choisir ses batailles, mettre les efforts au "bon" endroit
* √† tester = risque business + risque tech + facilement automatisable >= 1

-v-

## Processus

Notes:
* (reprendre des tamis) : tres amigos, example mapping, BDD, prise en compte de la testabilit√© d√®s la (pr√©-)conception, compter le co√ªt du test dans l'estimation de la story, les tests font partie de la dette technique du projet, analyse d'impact lors de nouveaux devs, d√©coupage en √©quipe Dev versus QA ??
  * identifier les manquements dans son √©quipe, sur son projet, et trouver comment communiquer dessus avec les autres, avoir des id√©es √† proposer

-v-

## Sc√©narios

Notes:
* sc√©narios de test (nominaux, critiques, ...) d√©cid√©s, "use cases" (orient√©s "utilisateur" de l'interface)
* TODO retravailler cette section, semble un peu redite avec la strat√©gie
* typologie de test : "unitaire au niveau technique (m√©thode/classe)", ou "unitaire d'interface mais profond"
* avoir une spec
  * Exigences et Requirements (cf Schneider, Thales, ...)
* value streams, risques, manque de confiance, ...
* smoke tests (cf origine du mot "smoke test" en logiciel)
* quoi tester ? critique ou sujet √† forte r√©gression, et stable, fr√©quence d'ex√©cution

-v-

## Architecture testable

Notes:
* sinon architecture intestable ou semi-testable
* seams (cf Michale feathers, Working effectively with legacy code) versus scalpel et pied-de-biche
* d√©pendances, interfaces, contrats, ... "trust boundaries"
* you can't write good tests for badly written code
* version de dev, suffisament isoprod mais avec des backdoors
* attention au couplage : ni trop peu, ni pas assez (monolithe spaghetti versus micro-services passe-plats)
  * plus simple de tester des fonctions (niveau code) que des programmes
* functionnal core, imperative shell (ou h√©xagonal, ou onion)
* pousser les IO √† l'ext√©rieur (technique du sandwich)
* design goal sinon accidentel
* crit√®re d'acceptabilit√©
* state is the enemy, prog fonctionnelle
* narrow interfaces for deep modules
* "fracture planes" de Team Topo, selon des "lignes de faille" (user persona, tech, change cadence, regulatory compliance, team location, risk, performance isolation, ...)
  * cf https://teamtopologies.com/key-concepts-content/finding-good-stream-boundaries-with-independent-service-heuristics
  * cf https://yoan-thirion.gitbook.io/knowledge-base/xtrem-reading/resources/book-notes/team-topologies#software-boundaries-or-fracture-planes
* sans architecture testable, la strat s'effondre !

-v-

## Surface d'interface

Notes:
* TODO: sous-partie de l'architecture testable ?
* ma√Ætriser la surface de test du code
* Tester aux fronti√®res d'une interface, que ce soit une m√©thode priv√©e, publique, classe, module, programme, sous-syst√®me, syst√®me, sur-systeme
* facile pour des modules narrow-interface mais deep, impossible pour des hubs
* les "unit" tests n'ont pas vocation √† tester le moinds de lignes possible, mais √† tester des APIs
* Ne pas tester tout le code ? Cf couverture, et faire des tests crois√©s
* Quantit√© de test versus qualit√©
* contravariance, tester l'interface plut√¥t que l'impl√©mentation, cf Uncle Bob https://blog.cleancoder.com/uncle-bob/2017/10/03/TestContravariance.html
* bon test = SRP + behavior not implementation
* niveau de test :
  * Test u = contrat dev
  * Test int√© = contrat int√©grateur
  * Test accep = contrat user
* Exercer le syst√®me depuis l'ext√©rieur
  * ok pour du e2e
  * un d√©sastre pour en tester des sous-parties (banane, gorille, jungle)
* "mock roles, not objects" - in "Growing Object-Oriented Software, Guided by Tests" by Steve Freeman and Nat Pryce

-v-

## Feedback

Notes:
* fast feedback + CI/CD + DevOps (monitoring, observability), frequent deployment, Monitoring, debuggability
* Feedback lors du dev, test, code review, design, recette, bugs en prod : tout renseigne sur ce qui m√©rite d'√™tre test√© et comment
* tester en prod avec le devops : canary, green-blue, ...
* Les tests doivent planter de temps en temps, pour les bonnes raisons
  * Signal et feedback
  * Doivent ne pas √©chouer tout le temps (sinon signe de couplage) ni jamais (signe que rien n'est test√©). Doivent √™tre un signal, ni z√©ro ni un. √âquilibre difficile.
* Les tests qui p√®tent (pour une bonne raison) c'est moins de bugs en prod, qui est l'objectif principal.

-v-

## Fluide

Notes:
* ajouter un test doit √™tre simple, s'il y a de la friction alors √ßa d√©courage de tester, il faut √©liminer la friction
* sentir la douleur : "If it hurts, do it more often" (core XP principle)
* ne pas √™tre capable de r√©aliser les tests rapidement diminue l'it√©rativit√©, la qualit√©, l'agr√©abilit√©, ... la probabilit√© qu'ils soient √©crit tout court
  * comme un √©vier plein de vaisselle (cercle vicieux)
* Blague du docteur : "j'ai mal quand je suis debout" "alors arr√™tez de vous lever"
* √©viter les micro-services, les submodules, les tests pas dans le m√™me repo que le code (tant que possible)
* Pente glissante de la qualit√©:
  * Si les tests sont difficiles √† lancer, ils ne le seront pas souvent
  * Si les r√©sultats des tests sont peu fiables/lisibles, il ne seront pas regard√©s
  * Si les tests sont lents √† s'√©x√©cuter, les tests rajout√©s seront lents aussi

-v-

## Investissement

Notes:
* investissement dans un second logiciel pour mieux produire le premier
* outils de test
* investir dans le futur
* projet logiciel = le code qui part en prod, mais pas seulement, aussi : la doc, la CI, les specs, et donc aussi les tests, ...
* "Le test n'apporte pas de valeur (argent) par rapport aux fonctionnalit√©s" (et les fonctionnalit√©s non plus d'ailleurs, √ßa d√©pend si elles sont connues, utilis√©es, ...)
* Second-syst√®me de stabilisation du premier, par opposition √† la flexibilit√©
* Une s√©curit√©, ou un frein ? les deux ?
* exemple de SQLite (x 590)
* Vraiment √† retenir : ne pas penser le test comme un apr√®s, mais comme un m√™me temps, voire enabler
* estimer le ROI de l'automatisation : gain versus co√ªt ? matrice
  * c'est un gros sujet pour les Test Managers, et les chefs de projets
* le meilleur moment pour commencer √† mettre des tests sur le projet c'√©tait le premier jour, le second meilleur moment c'est aujourd'hui
* https://xkcd.com/974/ "The General problem" (et sa caption : perception de perfectionniste versus ma√Ætre-artisan)
* https://xkcd.com/1205/ "Is it worth the time?"
* Le code pourrit sans test auto, mais les tests auto eux-m√™mes pourrissent, ou peuvent pourrir (v√©rouiller) le code
* J'avais un probl√®me, maintenant j'en ai deux (un nouveau)
* le test c'est une analyse co√ªt-b√©n√©fice : combien je veux mettre dans mes tests, pour le run de mon dev et de ma prod
* des tests syst√®mes pas effectu√©s = risque business
  * exemples : 8 bcs biologic, 50 bornes schneider, ...

-v-

## Ecriture

Notes:
* m√©thodologie d'√©criture : setup/teardown, Given/When/Then, Assert/Arrange/Act, tester une seule chose plut√¥t qu'un sc√©nario complet, erreur versus √©chec
* FIRST : https://stackoverflow.com/questions/18024785/tdd-first-principle Fast Indep Repeat Self-Check Timely (pas √©crit dans 1000 ans mais avec le code √† tester)
  * double i : isolation ?
  * test FIRST : Timely ou Thorough ?
* un test sans assert = red flag
* unit test = focus, sinon sc√©nario utilisateur end-to-end
  * sinon "Shotgun unit testing" : le test fait tout et n'importe quoi

-v-

## Techniques

Notes:
* ce qu'on consid√®re le minimum √† ma√Ætriser pour tester
* test harness
* TestContainers
* framework
* mocking et fakes versus simulateurs/√©mulateurs, oracles
  * qu'est-ce je gagne et je perds si je mocke ? gain = vitesse d'exec + facile √† mettre en oeuvre (autospec), perte = maintenabilit√© et r√©alisme
* DB in-memory

-v-

## techniques avanc√©es

Notes:
* outils et techniques <<avanc√©s>>, pour aller + loin (et qui m√©rite chacun son 45 minutes ou +) pour d√©velopper culture et savoir-faire
* TODO trier/ordonner ces techniques, d√©gager des cat√©gories ?
  * quelques-unes principales, d'autres justes √©nonc√©es ?
* advanced features of pytest (or your framework), know your tools
  * fixture
  * mock
  * plugins
* TU : D√©finition de test unitaire contre-intuitive : ne pas penser microscopique/indivisible, mais coh√©rence/s√©paration-fronti√®re
* TDD (cf on peut pas oublier de les faire √† la fin si on les fait au d√©but), diff√©rents sens du mot TDD, ...
  * cf slides ABC + Discovery Day
  * Mon tddd : testable design
  * "Tdd malgr√® son nom n'est pas une technique de test mais de design"
* mock et doublures : mock fake stub spy
  * Cf martinfowler (en ref √† la fin)
* chaos monkey + chaos engineering, cf Netflix + [Chaos Monkey Army](https://github.com/Netflix/SimianArmy/wiki/The-Chaos-Monkey-Army)
* ATDD versus BDD (parcours versus comportement segment√©, cf la Taverne)
  * Bdd mindset versus bdd outil
* snapshot/golden-master/approval
  * normatif versus descriptif
  * Approval Testing == Snapshot Testing == Golden Master ?
* fake time (freezegun en Python, libfaketime + LD_PRELOAD), Reactive instead of passive polling ou sleeping, 
* Ab testing
  * canary release, alpha testing, beta testing, blue-green, ...
  * Field testing (schneider)Dimensions de la pyramide : vitesse d'ex√©cution, co√ªt √† √©crire/maintenir, quantit√© de code exerc√©e, fid√©lit√© utilisateur, 
  * user acceptance ?
* sans io
* architecture h√©xagonale
* programmation fonctionnelle
* IA
* mesure de la couverture de test
  * Code coverage : line/branch/cond
  * 80 ? 90 ? 99 ?
  * Ne suffit pas !!!
  * Cf "1 / 0" avec 100% de coverage.
  * Ou "foo.update()" si foo est null
  * Il y a des branches invisibles (exceptions, donn√©es mal model√©es)
* test guid√© par la couverture (sans mention de pourcentage)
  * pour orienter les tests
* surveiller la performance des tests autos
  * ne correspond pas aux tests de performance
* security testing
* load testing (rendu accessible par de l'outillage, mais reste rare et hyper-sp√©cifique en terme de sc√©nario)
* table testing
* full simulation (√† la Matrix)
  * [What's the big deal about Deterministic Simulation Testing?](https://notes.eatonphil.com/2024-08-20-deterministic-simulation-testing.html)
  * [Pierre Zemb : Et si on faisait du simulation-driven development ?](https://www.youtube.com/watch?v=12LO_l90vDk)
* contract testing
  * Test d'interface d'une third-party (rupture d'API, ...)
* recording/replaying (VCR)
* monkey testing (Doublures de test : des inputs au hasard, le test n'est pas structur√©)
* profile your tests ! (√©viter les "slips/sleeps sales") cf snakeviz marche aussi pour les tests (cf article de Xavier et son setup de DB), tests en parall√®le (cf article du blog de PyPi), √™tre r√©actif plut√¥t que passif (cf MQTT tester de Schneider)
* property-based testing + fuzzing
* trunk-based development + feature flags
* tests de maintenabilit√© ISO 25010 (tests de modularit√©, r√©utilisabilit√©, analysabilit√©, modifiabilit√©, testabilit√©), cf https://latavernedutesteur.fr/2018/11/19/types-de-tests-iso-25-010-les-tests-de-maintenabilite-7-8/
* coverage-based testing
* mother object, method factories for test objects, data builders, ... https://martinfowler.com/bliki/ObjectMother.html
* parall√©lisation de tests
* dependency injection (D de SOLID), SRP
* DevOps (cf slide)
* test utilisateurices/accessibilit√©
* test de s√©cu (automatis√©)
* mutation testing
  * du code de prod, et du code de test
* Snapshot-testing (approval, golden master)
* Formal methods, preuves
* ISO/IEC 25010:2023
  * https://www.iso.org/fr/standard/78176.html
  * https://iso25000.com/images/figures/iso_25010_en.png
  * + crit√®res d'usage ISO 25019
* DDD
* boundary analysis et extreme values
* Page object model (POM)
* gestion des erreurs souvent peu pouss√©e, manque de contexte dans les logs
  * error model
* Technique de refactoring du sandwich (snowcamp) : push IO to the edge (functional core, imperative shell)
* Test d'√®chafaudage (scaffolding)
* Mockist vs Behaviorist (detroit vs london)
* (London "Mockist"/"Behaviorist" versus Detroit "Classicist")
  * exemple dans un post Linkedin : https://www.linkedin.com/posts/francois-baveye_met-tes-tests-unitaires-%C3%A0-la-poubelle-activity-7370443832401747968-Uc1O
  * tests unitaires : sociables vs solitaires (est-ce que les objets test√©s ont leurs d√©pendances r√©elles ou mock√©es) 
* tests d'architecture (Java = ArchUnit, Python = PyTestArch)
* Security testing : automated tools
* Clean Test = 3 things : readability, readability, readability
  * Evolving/surfacing a "testing language" to reveal intent
    * = POM ?
* Black box / white /glass
* Baseline perf comparison testing
* Pairwise pour la couverture, en contrant l'explosion combinatoire (produit cart√©sien des param√®tres)
* risk-based testing
* test data management
* "test impact analysis"
* Linter, typechecker, SonarQube, ... (compilation) des tests qu'il n'y a pas besoin d'√©crire (tests statiques)
  * Rust, tooling
* inclure des fonctionnalit√©s requises par les tests dans le code de prod ? non-pr√©f√©rable mais acceptable
* historisation (visuelle) des r√©sultats, pour rep√©rer les tendances, les patterns
* time : freezegun en Python, libfaketime avec LD_PRELOAD sinon
  * exemple : tester du code qui doit s'ex√©cuter pendant des mois (harness de test d'endurance)
* r√¥le de l'IA dans les tests ? (cf Tao blue/red team)
*  property vs fuzzing (cf [article sur la diff√©rence](https://www.tedinski.com/2018/12/11/fuzzing-and-property-testing.html))
  * oracle parfois difficile √† obtenir, parfois √©vident
* advanced features of pytest (or your framework), know your tools
* sans_io
* systrace/ptrace
* LD_PRELOAD
* HTTPS Man-in-the-Middle (MITM proxy par exemple)
* trucage DNS via `/etc/hosts` ou `/etc/resolv.conf`
* `ssl_verify=False`
* test des logs/metrics (cf mon post LinkedIn)
* test hybride : test auto avec v√©rif humD√©finition de test unitaire contre-intuitive : ne pas penser microscopique/indivisible, mais coh√©rence/s√©paration-fronti√®reaine, ou test manuel avec assistance autom
* anonymiser des donn√©es (de prod)
* g√©n√©rer des donn√©es

---

# 4. Cas pratique

Notes:
* TODO : 3 exemples de test
  * fonction pure (mais avec de la complexit√© interne), quelques cas d'erreur pr√©vus -> tests fonc, table, edge cases, fuzz, property-based
  * API WEB : mock, contrat, VCR, fake d'API
  * gros Postgres legacy, nouvelle feature

---

# 5. Conclusion

Notes:
* expertise indispensable, il faut s'y mettre, dans un environnement semi-hostile (vocab, √©quipe, rythme, outillage, ...) -> CI, run local. C'est une partie de l'ing√©nierie

---

# 6. Nos recommandations

Notes:
* TODO @jonathan d'autres √† rajouter ?
* [Jeremy Sorent "j'√©cris de tests sans pleurer maintenant" - LyonCraft 2025](https://www.youtube.com/watch?v=2S9TxoTE8BA) : TODO @julien mon avis
* [Michael feathers - Working effectively with legacy code](https://softwareengineering.stackexchange.com/questions/122014/what-are-the-key-points-of-working-effectively-with-legacy-code) : spoiler √ßa parle √©norm√©ment de test !
* [Joel "on Software" Spolsky - Hard-assed Bug Fixin‚Äô](https://www.joelonsoftware.com/2001/07/31/hard-assed-bug-fixin/) : est-ce que tous les bugs devraient √™tre corrig√©s ? √ßa d√©pend.
* [Mathieu Eveillard - 50 shades of tests](https://www.mathieueveillard.com/blog/50-shades-of-tests) : des d√©finitions plut√¥t claires pour diff√©rents types de test, leur positionnement sur 3 dimensions, au-del√† de la pyramide de tests
* [Marc Hage Chahine (La Taverne du Testeur) - Que doit-on attendre d‚Äôun testeur ?](https://latavernedutesteur.fr/2025/09/15/que-doit-on-attendre-dun-testeur/) : les diff√©rentes dimensions du m√©tier de testeur
* [Arnaud Lemaire - From code to consequences](https://www.youtube.com/watch?v=muRdH9u7gO4) : en quoi les "full cycle engineers" sont importants pour mener √† bien des projets
* [Colin Damon - ma typologie de tests et leur √©quilibrage](https://www.linkedin.com/posts/colin-damon_mettre-en-place-une-strat%C3%A9gie-de-tests-qui-activity-7343525861444247552-6BJY) : un exemple de "pyramide" dans un contexte pr√©cis
* [Dwayne Richard Hipp - How SQLite Is Tested](https://www.sqlite.org/testing.html) : un exemple de comment n'avoir quasi aucun bug pour un des logiciels les plus utilis√© au monde
* [Redowan Delowar - Test state, not interactions](http://rednafi.com/go/test-state-not-interactions/) : pourquoi les tests propos√©s par des LLMs ne sont pas n√©cessairement les bons, et comment faire mieux (par exemple privil√©gier les fakes aux mocks)
* [Jeff Atwood (CodingHorror) - Falling Into The Pit of Success](https://blog.codinghorror.com/falling-into-the-pit-of-success/) : comment ne plus avoir besoin de se battre pour que la qualit√© ne d√©gringole pas ?
* [Antoine Mazure - Tests pragmatiques : comment presque arreÃÇter les tests automatiseÃÅs ?](https://www.youtube.com/watch?app=desktop&v=ohV6GvCIeLY) : un exemple de tester la mauvaise chose, et de comment mieux tester avec pourtant moins de tests
* [Jules Poissonnet et Antoine Caron - Tester c'est tricher)](https://www.youtube.com/watch?v=I_zNxGqRI3w) : une vision d'ensemble, claire et illustr√©e, de la d√©marche de test, du vocabulaire et des difficult√©s
* [Christophe Br√©heret-Girardin - Comment une architecture influence votre strat√©gie de test ?](https://m.youtube.com/watch?v=IeOa6XWxkxg)
* [Ham Vocke - The Practical Test Pyramid](https://martinfowler.com/articles/practical-test-pyramid.html) : des exemples concrets dans un contexte clair de diff√©rents types de test, et des limitations de la pyramide de Mike Cohn
* [Martin Fowler - Mocks Aren't Stubs](https://martinfowler.com/articles/mocksArentStubs.html) : d√©finitions claires de tous les "*test doubles*" (dummy, stub, fake, spy, mock) par Martin Fowler
  * [Martin Fowler - Test Double](https://martinfowler.com/bliki/TestDouble.html) : en version ultra-abr√©g√©e
* [Ana√´l Lefebvre - Comment en finir avec la fragilit√© des tests unitaires](https://www.sqli.com/fr-fr/insights/comment-en-finir-avec-la-fragilite-des-tests-unitaires) : un contexte clair, une explication de FIRST, et une m√©thodo ("ZOMBIES") pour identifier les cas de test
* [Adam Bender - SMURF: Beyond the Test Pyramid](https://testing.googleblog.com/2024/10/smurf-beyond-test-pyramid.html) : un exemple par Google de d√©tricoter la pyramide des tests dans une vision compl√©mentaire des tests selontleurs propri√©t√©s techniques
* [Mi≈°ko Hevery - Writing Testable Code](https://testing.googleblog.com/2008/08/by-miko-hevery-so-you-decided-to.html) : un ensemble de conseils pour rendre son code testable, dont le premier point ("Mixing object graph construction with application logic") est trop m√©connu
* [The Grug Brained Developer - Testing](https://grugbrain.dev/#grug-on-testing) : des conseils de programmation pertinents, mais r√©dig√©s par "Grug" qui a une capacit√© limit√©e, et qui le revendique (!)
* [Qalisty - Comment s‚Äôen sortir lorsqu‚Äôon est 1 testeuse face √† 25 d√©veloppeurs ? (Ana√Øs Fournier)](https://open.spotify.com/episode/1nwA9nLdezVk6mzWu39T7a) : des techniques concr√®tes pour mettre en place une culture qualit√© et une strat√©gie
* [Victor Lambret - Le TDD sans commencer par les tests](https://www.youtube.com/watch?v=Ddarw3wUXQY) : TODO @julien mon avis
* [Mike Wacker - Just Say No to More End-to-End Tests](https://testing.googleblog.com/2015/04/just-say-no-to-more-end-to-end-tests.html) : les tests unitaires seraient ceux qui compte le plus, du point de vue des utilisateurs (!)
* [Simon Stewart - Test Sizes](https://testing.googleblog.com/2010/12/test-sizes.html) : caract√©risation des tests (en un tableau), non pas en unitaires versus end-to-end, mais en small versus big, en fonction de leurs IO
* [Igor Roztropi≈Ñski - Unit, Integration, E2E, Contract, X tests: what should we focus on?](https://binaryigor.com/unit-integration-e2e-contract-x-tests-what-should-we-focus-on.html) : de l'int√©r√™t de favoriser les tests d'int√©gration ("in-between", n'√©tant pas extr√™mes), et les tests de contrat
* [Kent C. Dodds - Write tests. Not too many. Mostly integration.](https://kentcdodds.com/blog/write-tests) : introduit la pyramide "trophy" (en incluant les tests statiques) pour des applis JS, avec surtout des tests d'int√©gration
* [Seb Rose - Making a meal of architectural alignment and the test-induced-design-damage fallacy](https://claysnow.co.uk/architectural-alignment-and-test-induced-design-damage-fallacy/) : une bonne le√ßon d'√©quilibre et de pragmatisme
* [IFTTD #43.src - Test: Tester c'est douter avec Arnaud Lemaire](https://open.spotify.com/episode/2gRex0ajRA1oVc7DZBL0B9) : TODO @julien
* [C√©cilia Bossard et Angi Guyard - On n‚Äôaurait pas oubli√© un truc dans le craft !?](https://www.youtube.com/watch?v=yVmKkRH60VI) : spoiler il s'agit des tests utilisateurs
* [BiteCode - Testing with Python (part 4): why and what to test?](https://www.bitecode.dev/p/testing-with-python-part-4-why-and) : toute la s√©rie d'articles vaut le d√©tour, mais cet √©pisode s'attarde sur, sans le nommer ainsi, la strat√©gie de test

---

# Bibliographie (et quantit√© pertinente)

* TODO @Julien Abseil
* TODO @Julien Software Craft (Dunod)
* TODO @Julien Clean Code 2008
* TODO @Julien Software Architecture in practice
* TODO @Julien A philosophy of software design
* TODO @Julien Working effectively with legacy code
  * (recommand√© auparavant)
* TODO @Julien Test-Driven Development By Example
* TODO @Julien It√©rations Product(ives)
* TODO @Julien Pourquoi votre strat√©gie de tests end-to-end √©choue ?
* Refactoring, Martin Fowler, 2√®me √©dition fran√ßaise, 2019 (410 pages)
  * Chapitre 4 "Cr√©ation des tests" (15 pages)

Manquants :
* TODO @Julien Agile Testing - Lisa Crispin et Janet Gregory
* Growing Object-Oriented Software Guided by Tests - ... TODO @julien
* Unit Testing - Baptiste Lyet TODO @julien
* How to test legacy code - emily bache TODO @julien
* Can we test it? Yes we can - Mitchell Hashimoto TODO @julien

---

# Cr√©dits photos

Notes:
* TODO later

---

# Remerciements

Notes:
* Damien Roulier, Eric Papazian, Mathieu Mattringe, Rachel Da Silva, Francky Flamant, Fanny Velsin, Victor Lambret

---

# Questions

Notes:
* TODO image

---

# Exemple de citation trompeuse

> "I get paid for code that works, not for tests [...]"

- Kent Beck

[...]

> I get paid for code that works, not for tests, so my philosophy is to test as little as possible to reach a given level of confidence (I suspect this level of confidence is high compared to industry standards, but that could just be hubris).
> If I don't typically make a kind of mistake (like setting the wrong variables in a constructor), I don't test for it
> I do tend to make sense of test errors, so I'm extra careful when I have logic with complicated conditionals.
> When coding on a team, I modify my strategy to carefully test code that we, collectively, tend to get wrong.

Source : [Stack Overflow en 2008 : "How deep are your unit tests?"](https://stackoverflow.com/a/153565/11384184)

---

# Abstract

L'enfer des tests auto

Des tests automatiques, on sait qu'il faut en faire. Mais trop souvent c'est une v√©ritable corv√©e : c'est lent, pas fiable, compliqu√© √† lancer, compliqu√© √† maintenir, voire compliqu√© √† √©crire tout court. Pourquoi c'est si dur ? Et comment reprendre le contr√¥le ?

Il va falloir reprendre depuis le d√©but : pourquoi on teste ? et on teste quoi ? et on teste comment ?
Ensuite, il va falloir revoir un peu les bases du test : un brin de vocabulaire, et surtout de la m√©thodologie (d√©pendances, interfaces, contrat, mock, simulateur, fixture, inversion, ...).
On fera un petit d√©tour par l'architecture, parce que la testabilit√© est une propri√©t√© √† prendre en compte d√®s le design, ou bien il faudra jouer du scalpel ou du pied-de-biche par la suite pour les faire rentrer.
Enfin, on passera √† l'impl√©mentation, et avec quelques bonnes pratiques et le bon outillage (framework de test, TestContainers, CI, ...), √ßa se passera plut√¥t bien.

A travers des cas concrets tir√©s de nos exp√©riences, suivez-nous sur le chemin pour quitter l'enfer des tests.
