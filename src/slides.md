# L'enfer des tests autos

Notes:
* TODO :
  * exemples concrets tout du long
  * exemples de code/archi Ã  la fin
  * images
  * FIXME: the slides should be vertical (cf mkslides_config.yml)
  * pimper la prÃ©sentation avec un peu de CSS (cf le mkslides_config.yml)

---

# 0. Introduction

Notes:
* TODO :
  * illustration de chapitre ?

-v-

## Merci aux sponsors

TODO

## PrÃ©sentation

Julien Lenormand ğŸ˜‡

Jonathan Gaffiot ğŸ˜ˆ
@ Kaizen Solutions

Notes:
* plutÃ´t qu'un disclaimer, penser Ã  expliciter qu'on a le choix entre :
  Ã©crire ses tests, ou tester manuellement, et bien souvent en prod
* on n'est pas parfait, des fois on ne teste pas (assez), ou pas auto

-v-

## Sondage

Notes:
* qui trouve que les tests c'est l'enfer ? Qui trouve que c'est le paradis ? le purgatoire ?

-v-


## C'est quoi tester ?

* c'est toujours difficile (et chiant !) les dÃ©finitions  <!-- .element: class="fragment" -->
* tester = s'assurer de la rÃ©ponse attendue de la part du systÃ¨me dans un certain Ã©tat Ã  un stimuli particulier  <!-- .element: class="fragment" -->
  * rÃ©ponse attendue ?  <!-- .element: class="fragment" -->
  * certain Ã©tat ?  <!-- .element: class="fragment" -->
  * quel stimulus ?  <!-- .element: class="fragment" -->
  * et les effets de bords de mon systÃ¨me ?  <!-- .element: class="fragment" -->

Notes:
* c'est quoi tester ? c'est quoi tester automatiquement ? (moment chiant avec des dÃ©finitions)
* action, rÃ©action, stimuli, SUT, oracle
  * dÃ©finiton test = s'assurer de la rÃ©ponse attendue de la part du systÃ¨me dans un certain Ã©tat Ã  un stimuli particulier
  * expliciter les piÃ¨ges :
    + rÃ©ponse attendue ? oui mais quand effet de bord ? LLM ? rÃ©sultat de simu ? (besoin d'un oracle)
    + certain Ã©tat ? c'est quoi les Ã©tats de mon systÃ¨me ? comment je mets mon systÃ¨me dans un Ã©tat particulier ?
    + quel stimulus ? quelle rÃ©ponse ? comment j'y accÃ¨de ?
    + et les effets de bords de mon systÃ¨me ? ses dÃ©pdendances Ã  d'autres systÃ¨mes ?

-v-

## Pourquoi tester ?

* QualitÃ© avec un grand Q :  <!-- .element: class="fragment" -->
    * ISO 9000 : aptitude d'un ensemble de caractÃ©ristiques intrinsÃ¨ques d'un objet (produit, service,...) Ã  satisfaire des exigences  <!-- .element: class="fragment" -->
    * Rambo Python : fiabilitÃ©, maintenabilitÃ©, Ã©volutivitÃ©, sÃ©curitÃ©  <!-- .element: class="fragment" -->
* Faire du logiciel ce n'est pas si simple  <!-- .element: class="fragment" -->

---

# 1. Pourquoi c'est important les tests autos ?

ğŸ˜‡ Pour garder la maitrise de son code au fur et Ã  mesure de son dÃ©veloppement !

ğŸ˜ˆ qui veut mettre en prod 2 ans de code jamais testÃ© ?

Notes:
* TODO :
<<<<<<< HEAD
  * illustration de chapitre ?
  * @Julien refaire une passe sur les rÃ©pÃ©titions (cf rÃ©union du mercredi 08 octobre)
=======
  * illustration de chapitre
>>>>>>> 89d9343 (apply review comments)

-v-

## Confiance

* ğŸ˜‡ vÃ©rifier que ce que j'ai changÃ© fonctionne correctement  <!-- .element: class="fragment" -->
* ğŸ˜‡ vÃ©rifier que ce que je n'ai pas changÃ© continue de fonctionner  <!-- .element: class="fragment" -->
* ğŸ˜‡ vÃ©rifier que l'ensemble fonctionne <!-- .element: class="fragment" -->
* ğŸ˜‡ mise en prod le vendredi !  <!-- .element: class="fragment" -->

Notes:
* sÃ©rÃ©nitÃ©

-v-

## Feedback rapide

* UN BUG ğŸ˜ˆ !  <!-- .element: class="fragment" -->
  - ğŸ˜ trouvÃ© lors de la PR  <!-- .element: class="fragment" -->
  - ğŸ˜© trouvÃ© lors des tests en prÃ©-prod  <!-- .element: class="fragment" -->
  - ğŸ˜¢ trouvÃ© en prod  <!-- .element: class="fragment" -->

* Un feedback rapide ğŸ˜‡ <!-- .element: class="fragment" -->
  - facile Ã  exÃ©cuter  <!-- .element: class="fragment" -->
  - rÃ©sultat rapide <!-- .element: class="fragment" -->
  - facile Ã  exploiter : log, stracktrace, dÃ©buggueur... <!-- .element: class="fragment" -->
  - indÃ©pendant, pas besoin de "QA"  <!-- .element: class="fragment" -->

Notes:
* ownership de la qualitÃ© du code, ce n'est pas juste aux QAs, ou utilisateurs de trouver les bugs, "Ã§a marche sur ma machine"
* facile Ã  exÃ©cuter : un clic et c'est bon, Ã§a part en prod

-v-

## FiabilitÃ© du code

* Le logiciel est une industrie particuliÃ¨re : complexitÃ©, fluiditÃ©, immatÃ©riel  <!-- .element: class="fragment" -->
  + complexitÃ© => chaque ligne est une action, effets de bord, combinatoire  <!-- .element: class="fragment" -->
  + immatÃ©riel => pas d'expÃ©rience immÃ©diate, visuelle, du systÃ¨me  <!-- .element: class="fragment" -->
  + fluiditÃ© => changements rapides et tout le temps  <!-- .element: class="fragment" -->
* ğŸ˜ˆ le code c'est trop dur pour vos petites cervelles d'humains !  <!-- .element: class="fragment" -->
* ğŸ˜‡ les tests aident Ã  rÃ©soudre ces problÃ¨mes :  <!-- .element: class="fragment" -->
  * on peut refactorer ou faire Ã©voluer le code en confiance  <!-- .element: class="fragment" -->
  * on a des preuves qu'il fonctionne correctement  <!-- .element: class="fragment" -->
  * d'autres personnes peuvent le modifier  <!-- .element: class="fragment" -->

-v-

* les tests sont un ingrÃ©dient pour la stabilitÃ© :  <!-- .element: class="fragment" -->
  * dans le temps  <!-- .element: class="fragment" -->
  * Ã  travers les technologies  <!-- .element: class="fragment" -->
  * malgrÃ© les Ã©volutions  <!-- .element: class="fragment" -->
  * pour dÃ©tecter les rÃ©gressions  <!-- .element: class="fragment" -->
  * survivre Ã  une absence imprÃ©vue (bus factor)  <!-- .element: class="fragment" -->
  * augmenter efficacement la taille de l'Ã©quipe  <!-- .element: class="fragment" -->
* pas de test automatisÃ© = risque projet  <!-- .element: class="fragment" -->

Notes:
* et les autres aspects de la qualitÃ© d'aprÃ¨s Rambo Python ? maintenabilitÃ©/Ã©volutivitÃ©/sÃ©curitÃ© !
* s'autoriser le refactoring, conserver du code maintenable
* stabilitÃ© / perennitÃ© / scalabilitÃ© humaine-temporelle-technique-complexitÃ©-busfactor
  * Tests auto = moyen de scaler dans le temps, la taille, les personnes, les techs, ...
  * pouvoir survivre Ã  un changement d'Ã©quipe
* quelle valeur Ã  un logiciel qui ne peut pas Ãªtre testÃ© automatiquement ? uniquement court-terme
* pas d'autom == risque projet
* une certaine forme de spÃ©cification (c'est plus simple de savoir ce que le code doit faire, quand c'est littÃ©ralement commitÃ© dans le repo)
    * et encore mieux, elle se vÃ©rifie toute seule !!
* exÃ©cution automatique/systÃ©matique -> pas besoin de devoir s'en souvenir (CI, make, ...)
* empÃªche le "Fear driven development"
  * Comment faire du fearless refactoring sans Rust ni test ?
* Ã©vite le "tombÃ© en marche"
* non-reg
* Sens strict de refactoring, pas de refactoring sans garantie que le comportement "observable" (externe) n'a pas Ã©voluÃ©
  * nÃ©cessaire pour dompter la dette technique

-v-

## Ã‰thique professionnelle

* le code peut Ãªtre une passion ğŸ˜‡   <!-- .element: class="fragment" -->
* ... et donc une torture ! ğŸ˜ˆ   <!-- .element: class="fragment" -->
* responsabilitÃ© perso/pro/presta  <!-- .element: class="fragment" -->
* "Ã©lever le niveau" - devise des crafteurs  <!-- .element: class="fragment" -->
* une question de maturitÃ© pro ?  <!-- .element: class="fragment" -->

Notes:
* cf Craft et dÃ©ontologie
* critÃ¨re de validitÃ© de ce qui est livrÃ© ("si c'est pas testÃ©, c'est rÃ©putÃ© ne pas marcher")
* pas obligÃ© de faire comme les autres
* livraison, recette

-v-

## RentabilitÃ©

* pas simple Ã  mesurer (scientifiquement)  <!-- .element: class="fragment" -->
* Accelerate ?  <!-- .element: class="fragment" -->
* argument d'autoritÃ© : Google, Microsoft, Netflix, Apple le font !!!!!!!!  <!-- .element: class="fragment" -->
  * et tous les projets libres qu'on utilise tous les jours !  <!-- .element: class="fragment" -->
* se concentrer sur des tÃ¢ches Ã  forte valeur ajoutÃ©e  <!-- .element: class="fragment" -->
* seul moyen de tenir la cadence  <!-- .element: class="fragment" -->

Notes:
* Accelerate
* TODO autres preuves d'efficacitÃ© ? (cf scientific proofs)
* se concentrer sur des activitÃ©s Ã  forte valeur ajoutÃ©e, par rapport Ã  rÃ©pÃ©ter des tests
* Seul moyen de tenir la cadence
* TODO: est-ce qu'il est vrai que les bugs coÃ»tent + cher Ã  corriger s'ils sont dÃ©couverts plus tard ? (preuves !!)

-v-

## Le paradis !

<<<<<<< HEAD
fin de la confÃ©rence ?  <!-- .element: class="fragment" -->

![](./10845744.jpg)  <!-- .element: class="fragment" -->
=======
* fin de la confÃ©rence ?  <!-- .element: class="fragment" -->
* TODO @julien retouche image  <!-- .element: class="fragment" -->
>>>>>>> 89d9343 (apply review comments)

Notes:
* une fois qu'on s'est dit Ã§a, Ã§a paraÃ®t vachement bien, donc y'a aucune raison de pas en faire
* meme avec l'image recto/verso, ville en feu, bÃ©bÃ© zombie
  * "Kid Thrown In The Air Meme: How Dad Sees It Vs How Mom Sees It" cf https://i.imgur.com/qL915f0.jpeg
* Stop au masochisme ! (TODO: autre section ?)
* transition vers la partie suivante

---

# 2. Pourquoi c'est difficile les tests autos ?

il faut bien l'avouer !

Notes:
* TODO: formuler les sections de faÃ§on Ã  rÃ©pondre au chemin de crÃªte ?
* TODO: ajouter des exemples concrets Ã  chacun

-v-

## Chemin de crÃªte

![](./StarorobociaÅ„ski_Wierch_a3.jpg)

Notes:
* mÃ©taphore du chemin de crÃªte : facile de tomber
<<<<<<< HEAD
* @Julien : une crÃªte a 2 versants, Ã§a donne l'image qu'on peut tomber de chaque cÃ´tÃ©.
  Ici on peut surtout tomber d'un cÃ´tÃ© (code qui a bien grandi, et pas de test),
  mais l'autre cÃ´tÃ© n'est pas symÃ©trique (trop de tests, du temps perdu, surqualitÃ© ?).
  Proposition de mÃ©taphore : tourner autour d'un trou noir. Tant que je fais l'effort,
  je reste en orbite, si je me relÃ¢che, je spirale vers le bas, et il faut bcp de boulot
  pour remonter.
=======
* TODO : fusionner avec slide au-dessus ?
>>>>>>> 89d9343 (apply review comments)

-v-

## Pas le temps

* Pas prÃ©vu dans le planning/sprint
* Mon chef/Product truc me dit de faire des features
* Deadline en vue
* Jamais budgetÃ©, jamais valorisÃ©
* Projet gÃ©rÃ© par le marketing sans aucune expÃ©rience de la technique

-v-

## Pas appris

Notes:
* sauf pour les testeurs de mÃ©tier, les moldus s'en passeront bien ?
* et encore les testeurs apprennent pas les TU
* pas de formation dans la plupart des cursus master, ou bien thÃ©orique ou trÃ¨s court
* assez peu prÃ©sent dans la littÃ©rature gÃ©nÃ©raliste, malgrÃ© sa prÃ©valence et importance (cf biblio)
* pas un sujet "sexy" (formation continue, confÃ©rences, ...)
* exemple Ensimag, mon cursus versus ce qui est proposÃ© actuellement
    * Mon Ã¨cole d'ingÃ©, ni mon DUT ne m'ont enseignÃ© le test
    * Le test Ã¨tait une option, une annÃ©e, d'une filiÃ¨re
    * les cours de Groz sur la vÃ©rif statique et la modÃ©lisation boite noire
    * Ensimag, examens de SAP en 2014 et 2015 :
      * dÃ©terminer les (in)variants de boucle
      * preuve d'arrÃªt,
      * interprÃ¨tation abstraite (rÃ©duction de machines Ã  Ã©tats),
      * accÃ¨s mÃ©moire invalide,
      * valeurs par dÃ©faut en Java,
      * dÃ©tection de la rÃ©cursion ou de code mort,
      * propagation d'intervalles pour exÃ©cution symbolique
      * preuves de comportement de programme
    * https://ensimag.grenoble-inp.fr/fr/formation/analyse-de-code-pour-la-s-ucirc-ret-eacute-et-la-s-eacute-curit-eacute-4mmacss
      * Ce cours est une introduction aux fondements de la sÃ©mantique et lâ€™analyse de programmes. Il offre les bases sur lesquelles sâ€™appuyer pour spÃ©cifier et dÃ©velopper des applications sÃ»res, construire et se servir dâ€™outils dâ€™analyse et de vÃ©rification.
      * SÃ©mantique opÃ©rationnelle des langages de programmation.
      * Calcul de plus faible prÃ©condition et preuve de programmes.
      * Analyse de flot de donnÃ©es.
      * Analyse statique et interprÃ©tation abstraite.
      * Applications Ã  la compilation, Ã  la sÃ»retÃ© et Ã  la sÃ©curitÃ© des logiciels.
      * Travaux pratiques Ã  l'aide de 2 outils industriels.
        * (Frama-C et Coq sÃ»rement ?)
    * https://ensimag.grenoble-inp.fr/fr/formation/test-des-syst-egrave-mes-logiciels-5mmtsl6
      * une option parmi 3, pour ceux qui font pas d'Erasmus
      * PrÃ©sentation des mÃ©thodes de test pour assurer la sÃ»retÃ© de fonctionnement des logiciels.
      * Test
      * VÃ©rification et validation
      * Les tests au cours du cycle de vie.
      * Test structurel des logiciels.
      * Test Ã  partir des spÃ©cifications: partitionnement, combinatoire.
      * MÃ©thodes de test basÃ©es sur des modÃ¨les, en particulier automates.
      * Analyse des notions de couverture, test mutationnel.
      * Eclairage sur des domaines de test importants:
        * test de performance et test de charge
        * test d'interface
        * test de sÃ©curitÃ©.
      * Cours de gÃ©nie logiciel abordant notamment les cycles de dÃ©veloppement : cela permet de situer correctement le test dans une activitÃ© de dÃ©veloppement.
      * Bonnes connaissances en algorithmique et programmation : Ãªtre capable d'analyser un programme, de l'exÃ©cuter symboliquement "Ã  la main", fait partie des activitÃ©s du testeur et est une compÃ©tence indispensable pour comprendre les techniques fondÃ©es sur l'analyse du code.
      * Langages et automates : une partie du cours porte sur de modÃ¨les et en particulier des machines d'Ã©tats finis exploitÃ©es pour engendrer des tests de conformitÃ©.
      * Bibliographie :
        * Aditya P. Mathur:Foundations of Spftware Testing, Pearson 2008.
        * J-F. Pradat-Peyre, J. Printz: Pratique des tests logiciels, Dunod 2009.
        * Myers, G.J. : The Art of Software Testing. Wiley 1979; rÃ©Ã©ditÃ© 2004.
    * le cours de derniÃ¨re annÃ©e
      * @Julien TODO zip de quentin pignÃ©
* "missing semester" ?
* Apprentissage thÃ©orique (en Ã©tudes ingÃ©) versus apprentissage empirique de l'informatique (sur le terrain), en particulier du test

-v-

## Vocabulaire confusant

Notes:
* personne n'est d'accord sur rien : 47 pyramides diffÃ©rentes, le vocabulaire du test,~~les perspectives tech~~, les rÃ´les, les niveaux de test
* lister et illustrer avec diffÃ©rents types de pyramides
    * blague illuminatis (pyramide)
    * On est des Ã¨gyptiens, on a plein plein de pyramides diffÃ¨rentes
* Pyramide originale par Mike Cohn dans "Succeeding with Agile" publiÃ© en 2009
* FlorilÃ¨ge des autres formes : ice cream, hourglass, diamond, upside-down pyramid, trophy (kent beck) ... --> aucune solution universelle
    * https://claude.ai/chat/3ff66008-4cc7-431e-9657-9f4987e7d86c
* Dimensions de la pyramide : vitesse d'exÃ©cution, coÃ»t Ã  Ã©crire/maintenir, quantitÃ© de code exercÃ©e, fidÃ©litÃ© utilisateur, ...
  * Multiples interprÃ©tations des dimensions de la pyramide : quantitÃ©, vitesse d'exÃ©cution, confiance, spÃ©cificitÃ©, couverture apportÃ©e, coÃ»t de production, frÃ©quence de changement / bug, ...
* Le test c'est un sujet transverse our chaque dev, qu'importe le langage, la stack, le mÃ©tier, ... Donc tout le monde parle de quelque chose avec un point de vue et un focus diffÃ¨rent, pas toujours mentionnÃ©
* prÃªt-Ã -penser
    * dans quels contextes-projets travaillent les diffÃ©rentes personnes qui ont proposÃ© ces pyramides ? et en quelle annÃ©e ?
    * ne suffit pas pour apprÃ©hender le test
    * Testing tools have evolved in 30 years
* jeu des 7314 diffÃ©rences : industrie, technos, maturitÃ© du produit, durÃ©e de maintenance, culture d'Ã©quipe, compÃ©tences lacunaires, vitesse, confiance, ...
    * il n'y en a pas qu'une seule, mais une par projet ! chaque projet est diffÃ©rent !
    * chaque projet est (quasi) unique
* anecdote sur le vocabulaire pas unifiÃ© : du temps de mon stage en autom de test, j'avais travaillÃ© entre autres sur un glossaire unifiÃ© entre ISTQB et CFTL
* meilleure dÃ©finition des axes : "finalitÃ©, granularitÃ©, modalitÃ© d'assertion" (50 shades ...)
* essayons de poser une dÃ©finition, dans notre contexte, des mots que nous employons
    * diffÃ©rences entre ISTQB et CFTL, cf mon stage Sogeti
* tous les diffÃ©rents types de test : carac, fonc, integ, unit, acceptance, user-testing, accessibility, composant-unitaire versus composant-UI, e2e-stack ou e2e-scÃ©nario...
    * TODO more
    * charge : soak, breakpoint, ... (cf doc de k6 avec de jolis graphiques)
        * K6 typologie : breakpoint, soak, stress, load, ...
    * cf ISO 15010
    * peu de personnes, mÃªme dans des livres (exemple Clean Code de Robert C Martin "Uncle Bob"), ne font l'effort de dÃ©finir
    * Pyramyde inversÃ© de la valeur : le haut niveau est le plus pertinent, mais cher et fragile ?
        * Facilitation des tests web d'ui (selenium, cypress, playwright)
* QA analyst/tester vs quality engineer vs testeur-automaticien vs "dev" vs IVVQ vs QA "quality advisor" vs test manager ...
* Test error vs test failure : le test n'a pas abouti pour une raison technique (problÃ¨me de test), le test a abouti mais n'a pas produit le rÃ©sultat attendu (problÃ¨me fonctionnel on espÃ¨re !)
  * distinction "test qui plante" versus "test qui Ã©choue"
* Imbrication et interconnexion des systÃ¨mes : les tests au mÃªme endroit n'ont pas le mÃªme nom / fonction.
* politique vs stratÃ©gie vs plan de test
* E2e : vertical de la stack ? Ou horizontal du parcours utilisateur ?
* Sanity vs smoke ?
* test fonctionnel + non-fonctionnel : si la perf fait partie des requirements, alors fonctionnel ou pas ?
* [le glossaire ISTQB](https://glossary.istqb.org/en_US/search?term=) donne 601 rÃ©sultats en anglais, 559 en franÃ§ais
* personne n'est d'accord sur les tests, car personne n'utilise les mÃªmes dÃ©finition
  * dÃ©monstration : pain au chocolat

-v-

## Trop de tests

Notes:
* explosion combinatoire
* cible mouvante et floue
* L'explosion combinatoire rend l'exhaustivitÃ© impossible

-v-

## Pratique rÃ©ticente

Notes:
* des tonnes d'outils diffÃ©rents, les diffÃ©rents types de tests Ã©voquÃ©s, les diffÃ©rents mÃ©tiers, l'insertion dans le process de production, ...
* s'organiser, planifier et rÃ©aliser sont des tÃ¢ches complexes

-v-

## Punition

Notes:
* l'absence de testeur/expertise/culture dans les projets (des gens formÃ©s, motivÃ©s, avec le mindset adÃ©quat)
* > On peut conduire un cheval Ã  l'abreuvoir, mais pas le forcer Ã  boire
* certifications ISTQB par le CFTL
* casse-pied de service

-v-

## Inutile

Notes:
* convaincre (le management et/ou les devs) que c'est utile, avant de se manger une mise-en-prod foirÃ©e
* dÃ©pense versus Ã©conomie
* RÃ©sultats intangibles

-v-

## Test impossible

Notes:
* imitations techniques, matÃ©rielles, de coÃ»t, ... variabilitÃ© selon les environnements, pas de donnÃ©es de test (rÃ©alistes)
* exemple board farm Schneider
* exemple Windows 10 LTSC 2019 Ã  Thales

-v-

## Code intestable

Notes:
* Ennemis : side-effects ("spooky action at a distance", "que fait cette mÃ©thode ?"), (global/static) state, IO, singletons, time, locale, network, files, env, GPU, unclear pre/post-conditions, non-determinism, (G)UI vs API, concurrency et threading, random (non-deterministic), complex outputs and high dimensionality
* diffÃ©rence entre "c'est compliquÃ© de rÃ©aliser le test" (limitations tech) versus "c'est compliquÃ© d'Ã©crire le test" (iceberg, gorille)
* exemple : code des bornes qui Ã©choue le 29 FÃ©vrier
* exemple : code du Edge qui est correct sur la timezone 6 mois par an
* anecdote schneider :
  * on me demande de rajouter un paramÃ¨tre boolÃ©en Ã  une route HTTP, sur un composant sur lequel je n'ai jamais travaillÃ©
  * je me dis je vais le faire en TDD, commencer par Ã©crire un test
  * mais y'a aucun test sur le projet, donc je dois commencer par Ã©crire un test avant
  * j'Ã©cris un test, je le lance un fois il passe, deux fois il Ã©choue
  * car il avait une dÃ©pendance sur une base de donnÃ©es, laquelle est gÃ©rÃ©e par un singleton, qui stocke l'Ã©tat
  * donc je fixturise le singleton pour le rÃ©initialiser (ou le dÃ©truire/reconstruire) afin que mon test soit rÃ©pÃ©table
  * ensuite j'ajoute mon test et le changement est easy
  * (ah, maintenant faut que j'ajoute une CI !!)

-v-

## Fragile

Notes:
* couplage versus maintenabilitÃ© en carton, tests cassÃ©s pas rÃ©parÃ©s ("vitre cassÃ©e"), maintenance des tests vÃ©cue comme un fardeau
  * le summum : test de mock !
* Tester le mauvais 80% : mÃ©taphore du streetlight problem ("plus simple de chercher dans la lumiÃ¨re")
* test flaky
  * l'Ã©quivalent de "ptÃ¨t bin qu'oui, ptÃ¨t bin qu'non"
  * suffit de les relancer plusieurs fois mdr, ~zÃ©ro confiance

-v-

## Illisible

Notes:
* exemple d'Eric (Schneider)

-v-

## Lent

Notes:
* lenteur, car tests lents et/ou trop de tests
* exemple de Xavier Nopre (cf [post LinkedIn](https://www.linkedin.com/posts/xnopre_pourquoi-jai-mis-plus-de-3-jours-%C3%A0-trouver-activity-7316027544934191104-Otww)), je lui ai dit qu'il y avait un problÃ¨me, c'est pas censÃ© Ãªtre aussi lent
* exemple de Schneider : board farms
* exemple de Schneider : code dont le run dure des centaines jours !! (Ã  travers les timezones :p)

-v-

## Bug ou feature ou code mort ?

Notes:
* descriptivism vs prescriptivism (cf Romeu)
* test de caractÃ©risation OK, mais est-ce que c'est ce que Ã§a devrait vraiment faire ? ğŸ¤·
  * bug ou feature ?
  * xkcd workflow https://xkcd.com/1172/
* source de vÃ©ritÃ© = code ou spec Word ?

-v-

## L'enfer !

Notes:
* difficile de savoir quoi faire, comment faire, de le faire, Ã  exÃ©cuter, Ã  analyser, Ã  maintenir, Ã  faire confiance, pas assez, pas assez bien, pas assez rapide, ...
* Facile Ã  dire de faire "le bon test", mais concrÃ¨tement ?
  * y'a le bon testeur et le mauvais testeur ...
* transition

---

# 3. Comment faire pour bien tester auto ? (il faut s'aider !)

Notes:
* TODO:
  * orientÃ© solution cf partie prÃ©cÃ©dente orientÃ© problÃ¨me ?
  * le formuler de faÃ§on Ã  rÃ©pondre au chemin de crÃªte
    * osef ?
  * @julien Ã  chaque section mettre un exemple !!!

-v-

## Chemin de crÃªte

Notes:
* TODO remettre la photo ?
* comment le naviguer ? comment Ãªtre/devenir/rester rigoureux ?
  * discipline d'un moine bouddhiste, ou d'un garde de la reine d'Angleterre

-v-

## Culture de la qualitÃ©

> Culture eats strategy for breakfast. -- Peter Drucker (apocryphe !!)

* il faut avant tout changer la Culture Ã©quipe/projet
  * implication des stakeholders
  * dÃ©marche d'Ã©quipe, pas juste avoir un casse-pied de service
  * tournure d'esprit requise pour malmener le code ("vicieux")
    * "un testeur rentre dans un bar, il commande ..."
  * humilitÃ©
  * responsabilitÃ© individuelle + Ã©quipe"
* tamis successifs pour attraper les bugs : du besoin Ã  la prod, et toutes les Ã©tapes intermÃ©diaires
* avoir de l'expÃ©rience est un vrai plus

-v-

Petit florilÃ¨ge :

> Le test n'apporte pas de valeur (argent) par rapport aux fonctionnalitÃ©s

> you get paid for "software", not "maintainable software" -- joncroks, https://news.ycombinator.com/item?id=13130991

> Move fast and break things -- Facebook (jusqu'en 2014)

> I get paid for code that works, not for tests [...] -- Kent Beck (tronquÃ© !!)

Notes:
* culture qualitÃ©, formation (cours, confÃ©rences, livres, exercices, katas, ...)
* cf nos recos Ã  la fin, expÃ©rience (empirisme)
* comprÃ©hension du business et des stakeholders,
* tournure d'esprit (cf joke "un testeur rentre dans un bar, il commande -1 biÃ¨re, NaN biÃ¨re, demande oÃ¹ sont les toilettes ..."), "vicieux" pour "casser le code" et non pas seulement montrer qu'il fonctionne
* exemple : SQLite testing, test code ratio, test harnesses, ... (+Ã©volution dans le temps)
* le code il faut le malmener
* la qualitÃ© c'est un ensemble de tamis successifs : empilement de couches pour attraper les bugs, du besoin, design, archi, implem, test, validation, dÃ©ploiement
  * processus/dÃ©marche au niveau de l'Ã©quipe/projet/entreprise/...
* exemple : NDP Systems
* "Le test n'apporte pas de valeur (argent) par rapport aux fonctionnalitÃ©s"
* HumilitÃ© de devoir tester
* "move fast and break things"
* https://news.ycombinator.com/item?id=13130991 : you get paid for "software", not "maintainable software"
* Avoir un "Patrick" (ou whatever name) dans son Ã©quipe, Ã©ternellement vigilant, le "relou"
* comme beaucoup de sujets, c'est pas un casse-pied de service qu'il faut, mais un changement de culture (beaucoup plus compliquÃ©, cf Agile bullshit, sÃ©curitÃ©, ...)
* ResponsabilitÃ© individuelle et d'Ã©quipe
* la qualitÃ© c'est une dÃ©marche, un tamis, un empilement (vrai sens de Kanban), une culture (LEAN, Kanban "right the first time")
* market cap de Facebook en 2014 >= 140 milliards de $

-v-

## StratÃ©gie de test

Notes:
* quoi pourquoi pour quoi comment qui quand ...
* spÃ©cifications, Exigences et Requirements, business (value stream) et risques business, quadrants, matrice confiance versus risque, moyens (et toute la suite)
  * cartographier les flux dâ€™information, les cas dâ€™usage, les frontiÃ¨res techniques et les domaines de lâ€™application afin de discerner les frontiÃ¨res des tests
* construisez votre "pyramide" (demander Ã  une IA diffÃ©rente pyramides, de plein de formes diffÃ©rentes Ã  la Dali), en se basant sur les moyens de test
* Ã  adapter/repenser rÃ©guliÃ¨rement, comme tout le reste
* pourquoi, pour quoi, quoi, oÃ¹, qui, quand et comment, ...
* trust boundaries (dÃ©pendences externes, parties peu fiables, risque mÃ©tier, responsabilitÃ©, vitesse d'Ã©volution, ...)
* Repenser la strat frÃ©quemment, tout comme on le fait pour l'architecture de la solution
* considÃ©rer les niveaux de test : FonctionnalitÃ©s techniques (endpoint) versus user
* Quadrant des tests !! Axes : business vs tech, pour le produit vs Ã¨quipe
* dÃ©pendences externes (on ne les controle pas, c'est compliquÃ©, et elles peuvent changer sans qu'on y prÃªte attention) versus dÃ©pendances internes (on les controle, mais c'est quand mÃªme compliquÃ©, et peuvent changer si on ne fait pas assez gaffe)
* Choisir l'effort de test par pÃ©rimÃ¨tre = choisir lÃ  oÃ¹ on prÃ©fÃ¨re diminuer la proba d'un bug
* contrats (boundaries)
* Plan de test au format IEEE 29119-3 (ou pas !)
* Le systÃ¨me testÃ© peut faire partie d'un sur-systÃ¨me, et se composer de sous-systÃ¨mes, les composants des uns sont les acceptation des autres
  * Ce qui dÃ©pend de nous versus ce qui ne dÃ¨pend pas (stoÃ¯cisme)

-v-

## Moyens de test

> Pas de bras, pas de chocolat ! -- Omar Sy

* Pas de spec ...
* Pas le temps ...
* Pas les compÃ©tences ...
* Pas les outils ...
* Pas le hardware ...
* Pas les moyens de dÃ©ployer ...
* Pas de data de test ...

Notes:
* humains, techniques, temporels, ...
* outils de test, formations, avoir le temps, dÃ©ployabilitÃ©, disponibilitÃ© du hardware, dispo des data, ...
* avoir des specs ! (claires)

-v-

## Renoncer

> Choisir, c'est renoncer -- citation fausse d'AndrÃ© Gide

Notes:
* renoncer Ã  tout automatiser (quadrants, moyens insuffisants, ...), ROI
* tradeof : cout, risque, complexitÃ©, ...
* critÃ¨re qui favorisent l'automatisation : rÃ©pÃ©tition, confiance dans l'autom, pÃ©nibilitÃ©, longueur, criticitÃ©, ...
* il vaut mieux un mix d'autom et de manuel, la force de chacun
* Le coÃ»t des tests est parfois supÃ©rieur aux benÃ©fices
* exemple : lecteur de fichier Ã  EDF
* Tester le code techniquement complexe, sensible pour le mÃ©tier, et utile
* loi de pareto 80/20 : toujours fausse, toujours vraie
* hybrid possible aussi (soit manuel assistÃ© par autom, soit autom avec verif manuelle) : continuum AutomatisÃ©-automatisable-manuel
* Sans jugement : un pas aprÃ¨s l'autre, on hÃ©rite de codebases, on essaye de faire mieux
  * incrÃ©mental
* tester ne prouve pas l'absence de bugs, mais en Ã©limine certains
* process avec des rendements dÃ©croissants, trouver le bon curseur, le bon Ã©quilibre
* garder des tests qu'on apprÃ©cie : rapides (ou moins rapides en CI mais + couvrants), fiables, maintenables, estimer le ROI
* OK de supprimer un test inutile
* fatalisme, tout tester en auto est un voeu pieux
* Choisir ses batailles, mettre les efforts au "bon" endroit
* Ã  tester = risque business + risque tech + facilement automatisable >= 1

-v-

## Processus

Notes:
* (reprendre des tamis) : tres amigos, example mapping, BDD, prise en compte de la testabilitÃ© dÃ¨s la (prÃ©-)conception, compter le coÃ»t du test dans l'estimation de la story, les tests font partie de la dette technique du projet, analyse d'impact lors de nouveaux devs, dÃ©coupage en Ã©quipe Dev versus QA ??
  * identifier les manquements dans son Ã©quipe, sur son projet, et trouver comment communiquer dessus avec les autres, avoir des idÃ©es Ã  proposer

-v-

## ScÃ©narios

Notes:
* scÃ©narios de test (nominaux, critiques, ...) dÃ©cidÃ©s, "use cases" (orientÃ©s "utilisateur" de l'interface)
* TODO retravailler cette section, semble un peu redite avec la stratÃ©gie
* typologie de test : "unitaire au niveau technique (mÃ©thode/classe)", ou "unitaire d'interface mais profond"
* avoir une spec
  * Exigences et Requirements (cf Schneider, Thales, ...)
* value streams, risques, manque de confiance, ...
* smoke tests (cf origine du mot "smoke test" en logiciel)
* quoi tester ? critique ou sujet Ã  forte rÃ©gression, et stable, frÃ©quence d'exÃ©cution

-v-

## Architecture testable

Notes:
* sinon architecture intestable ou semi-testable
* seams (cf Michale feathers, Working effectively with legacy code) versus scalpel et pied-de-biche
* dÃ©pendances, interfaces, contrats, ... "trust boundaries"
* you can't write good tests for badly written code
* version de dev, suffisament isoprod mais avec des backdoors
* attention au couplage : ni trop peu, ni pas assez (monolithe spaghetti versus micro-services passe-plats)
  * plus simple de tester des fonctions (niveau code) que des programmes
* functionnal core, imperative shell (ou hÃ©xagonal, ou onion)
* pousser les IO Ã  l'extÃ©rieur (technique du sandwich)
* design goal sinon accidentel
* critÃ¨re d'acceptabilitÃ©
* state is the enemy, prog fonctionnelle
* narrow interfaces for deep modules
* "fracture planes" de Team Topo, selon des "lignes de faille" (user persona, tech, change cadence, regulatory compliance, team location, risk, performance isolation, ...)
  * cf https://teamtopologies.com/key-concepts-content/finding-good-stream-boundaries-with-independent-service-heuristics
  * cf https://yoan-thirion.gitbook.io/knowledge-base/xtrem-reading/resources/book-notes/team-topologies#software-boundaries-or-fracture-planes
* sans architecture testable, la strat s'effondre !
* introduire des interfaces au bon endroit pour casser la combinatoire (passer de la multiplicaton des cas Ã  l'addition)

-v-

## Surface d'interface

> good cut point has narrow interface with rest of system: small number of functions or abstractions that hide complexity demon internally, like trapped in crystal
> -- grugbrain.dev

Notes:
* TODO: sous-partie de l'architecture testable ?
* un critÃ¨re primordial pour faciliter la testabilitÃ© : maÃ®triser la surface (de test) du code
* Tester aux frontiÃ¨res d'une interface, que ce soit une mÃ©thode privÃ©e, publique, classe, module, programme, sous-systÃ¨me, systÃ¨me, sur-systeme
* facile pour des modules narrow-interface mais deep, impossible pour des hubs
* les "unit" tests n'ont pas vocation Ã  tester le moinds de lignes possible, mais Ã  tester des APIs
* Ne pas tester tout le code ? Cf couverture, et faire des tests croisÃ©s
* QuantitÃ© de test versus qualitÃ©
* contravariance, tester l'interface plutÃ´t que l'implÃ©mentation, cf Uncle Bob https://blog.cleancoder.com/uncle-bob/2017/10/03/TestContravariance.html
* bon test = SRP + behavior not implementation
* niveau de test :
  * Test u = contrat dev
  * Test intÃ© = contrat intÃ©grateur
  * Test accep = contrat user
* Exercer le systÃ¨me depuis l'extÃ©rieur
  * ok pour du e2e
  * un dÃ©sastre pour en tester des sous-parties (banane, gorille, jungle)
* "mock roles, not objects" - in "Growing Object-Oriented Software, Guided by Tests" by Steve Freeman and Nat Pryce

-v-

## Feedback

* les tests sont les premiers utilisateurs de notre code
  * du code peu testable se voit immÃ©diatement ! et Ã§a se propage !
* A la fin du dev, il est parfois trÃ¨s tard pour corriger le tir : Shift Left
  * il fallait le prendre en compte lors de l'implÃ©mentation, du design, du poker, de la story ...
* A la fin du dev, il est parfois trop tÃ´t pour corriger le tir : Shift Right
  * Ã§a part en prod en on surveille (feature toggle, monitoring, metrics)
  * (beaucoup plus sÃ©rieux que dire "je teste en prod")
* le test est une considÃ©ration tout du long du process : TestOps, Full Cycle
  * Ã©viter la loi de Conway : les testeurs testent, les autres s'en fichent (cf Culture)

-v-

* un test qui pÃ¨te, c'est une bonne nouvelle : un bug de moins en prod !
* les bugs ne sont pas que dans le code, mais aussi dans la code review, la spec, le process, les communications ...
* les bugs ne sont pas que des erreurs mais aussi des occasions d'amÃ©liorer
  * (beaucoup plus sÃ©rieux que dire "c'est pas un bug mais une feature")
* un signal (au sens de la "thÃ©orie de l'information")
* le bon test : il ne passe tout le temps ni n'Ã©choue tout le temps, il Ã©choue pour les bonnes raisons
  * le bon chasseur ...

Notes:
* fast feedback + CI/CD + DevOps (monitoring, observability), frequent deployment, Monitoring, debuggability (shift right)
* Feedback lors du dev, test, code review, design, recette, bugs en prod : tout renseigne sur ce qui mÃ©rite d'Ãªtre testÃ© et comment (shift left)
* tester en prod avec le devops : canary, green-blue, ...
* Les tests doivent planter de temps en temps, pour les bonnes raisons
  * Signal et feedback
  * Doivent ne pas Ã©chouer tout le temps (sinon signe de couplage) ni jamais (signe que rien n'est testÃ©). Doivent Ãªtre un signal, ni zÃ©ro ni un. Ã‰quilibre difficile.
* Les tests qui pÃ¨tent (pour une bonne raison) c'est moins de bugs en prod, qui est l'objectif principal.
* luter contre la "loi de conway" : les devs versus les testeurs, cf Full Cycle
* pas de "Ã§a marche sur ma machine"
* du code difficile Ã  tester va engendrer + de tests fragiles, sans amÃ©liorer le design, qui va empirer, et ainsi de suite

-v-

## Fluide

Pente glissante de la qualitÃ© :
* Si les tests sont difficiles Ã  lancer, ils ne le seront pas souvent, de moins en moins
* Si les rÃ©sultats des tests sont peu fiables/lisibles, ils ne seront pas beaucoup regardÃ©s, de moins en moins
* Si les tests sont lents Ã  s'exÃ©cuter, les tests rajoutÃ©s seront lents aussi, de plus en plus

> Docteur, quand j'appuie lÃ , j'ai mal !
> Alors n'appuyez pas lÃ .
> -- blague

-v-

> If it hurts, do it more often.
> -- core XP principle

Identifier les "pain points" et les rÃ©soudre :
* ajouter un test doit Ãªtre simple et rapide
* lancer les tests doit Ãªtre simple et rapide

Corolaire : Ã©viter (ant que possible)

* les micro-services
* les submodules
* les tests dans un repo Ã  part
* ...

Notes:
* ne pas Ãªtre capable de rÃ©aliser les tests rapidement diminue l'itÃ©rativitÃ©, la qualitÃ©, l'agrÃ©abilitÃ©, ... la probabilitÃ© qu'ils soient Ã©crit tout court
  * comme un Ã©vier plein de vaisselle (cercle vicieux)

-v-

## Investissement

* une suite de tests autos est un logiciel, dont le but est de vÃ©rifier le bon fonctionnement d'un autre
* il s'agit d'un second systÃ¨me, qui sert Ã  stabiliser le premier
  * il n'apporte pas de valeur en soi, mais aide le premier Ã  continuer d'en apporter (comme la doc, la CI, le marketing, ...)
  * il nÃ©cessite de l'investissement (temps, compÃ©tences, argent, ...)
  * le meilleur moment pour investir dedans, c'Ã©tait hier, le second meilleur c'est aujourd'hui
  * il est d'autant plus rentable qu'on l'utilise (enabler !)
  * (il peut Ãªtre une gÃªne si on prÃ©fÃ¨re une extrÃªme flexibilitÃ© : sÃ©curitÃ© versus frein)
  * il engendre des risques
  * analyse coÃ»t-bÃ©nÃ©fice, ROI (return on time invested)
* exemple (extrÃªme !) de SQLite : 590x plus de code de test que de code de prod

-v-

![xkcd 974 "The General problem"](./the_general_problem.png)

> I find that when someone's taking time to do something right in the present, they're a perfectionist with no ability to prioritize, whereas when someone took time to do something right in the past, they're a master artisan of great foresight.

![xkcd 1205 "Is it worth the time?"](./is_it_worth_the_time.png)

Notes:
* investissement dans un second logiciel pour mieux produire le premier
* outils de test
* investir dans le futur
* projet logiciel = le code qui part en prod, mais pas seulement, aussi : la doc, la CI, les specs, et donc aussi les tests, ...
* "Le test n'apporte pas de valeur (argent) par rapport aux fonctionnalitÃ©s" (et les fonctionnalitÃ©s non plus d'ailleurs, Ã§a dÃ©pend si elles sont connues, utilisÃ©es, ...)
* Second-systÃ¨me de stabilisation du premier, par opposition Ã  la flexibilitÃ©
* Une sÃ©curitÃ©, ou un frein ? les deux ?
* exemple de SQLite (x 590)
* Vraiment Ã  retenir : ne pas penser le test comme un aprÃ¨s, mais comme un mÃªme temps, voire enabler
* estimer le ROI de l'automatisation : gain versus coÃ»t ? matrice
  * c'est un gros sujet pour les Test Managers, et les chefs de projets
* le meilleur moment pour commencer Ã  mettre des tests sur le projet c'Ã©tait le premier jour, le second meilleur moment c'est aujourd'hui
* https://xkcd.com/974/ "The General problem" (et sa caption : perception de perfectionniste versus maÃ®tre-artisan)
* https://xkcd.com/1205/ "Is it worth the time?"
* Le code pourrit sans test auto, mais les tests auto eux-mÃªmes pourrissent, ou peuvent pourrir (vÃ©rouiller) le code
* J'avais un problÃ¨me, maintenant j'en ai deux (un nouveau)
* le test c'est une analyse coÃ»t-bÃ©nÃ©fice : combien je veux mettre dans mes tests, pour le run de mon dev et de ma prod
* des tests systÃ¨mes pas effectuÃ©s = risque business
  * exemples : 8 bcs biologic, 50 bornes schneider, ...

-v-

## Ecriture

quelques rÃ¨gles d'Ã©criture pour les tests :

* setup et teardown pour prÃ©parer/dÃ©comissionner les ressources nÃ©cessaires
* structure du test en Assert-Arrange-Act (ou Given-When-Then)
* au moins un assert (ou composite)
* une seule action ou plusieurs actions ?
* FIRST = Fast, Independant, Repeatable, Self-Checking, Timely
* diffÃ©rencier "erreur" (pas de rÃ©sultat de test) versus "Ã©chec" (rÃ©sultat nÃ©gatif)

Notes:
* mÃ©thodologie d'Ã©criture : setup/teardown, Given/When/Then, Assert/Arrange/Act, tester une seule chose plutÃ´t qu'un scÃ©nario complet, erreur versus Ã©chec
* FIRST : https://stackoverflow.com/questions/18024785/tdd-first-principle Fast Indep Repeat Self-Check Timely (pas Ã©crit dans 1000 ans mais avec le code Ã  tester)
  * double i : isolation ?
  * test FIRST : Timely ou Thorough ?
* un test sans assert = red flag
* unit test = focus, sinon scÃ©nario utilisateur end-to-end
  * sinon "Shotgun unit testing" : le test fait tout et n'importe quoi

-v-

## Techniques

le minimum Ã  maÃ®triser selon nous :

* mettre en place un "harnais de test" (environnement testable)
* utiliser un framework
* savoir utiliser des fakes (in-memory), des mocks, des TestContainers, ...
* comprendre les consÃ©quences de ses dÃ©cisions (vitesse, fiabilitÃ©, facilitÃ©, maintenabilitÃ©)

Notes:
* @TODO jonathan est-ce que tu vois autre chose Ã  rajouter ?
* ce qu'on considÃ¨re le minimum Ã  maÃ®triser pour tester
* test harness
* TestContainers
* framework
* mocking et fakes versus simulateurs/Ã©mulateurs, oracles
  * qu'est-ce je gagne et je perds si je mocke ? gain = vitesse d'exec + facile Ã  mettre en oeuvre (autospec), perte = maintenabilitÃ© et rÃ©alisme
* DB in-memory

-v-

## Techniques avancÃ©es

* snapshot/golden-master/approval-testing
* property-based testing + fuzzing
* tests d'architecture
* IA
* Page Object Model (POM) pour les tests d'UI
* Accelerate
* ...
* juste le sommet de l'iceberg (le reste dans les sources)

Notes:
* pour aller + loin (et qui mÃ©rite chacun son 45 minutes ou +) pour dÃ©velopper culture et savoir-faire
  * types de test
    * TU : DÃ©finition de test unitaire contre-intuitive : ne pas penser microscopique/indivisible, mais cohÃ©rence/sÃ©paration-frontiÃ¨re
      * 3 axes :
        * vÃ©rification de la valeur de retour
        * vÃ©rification de l'Ã©tat
        * vÃ©rification de la collaboration
    * ATDD versus BDD (parcours versus comportement segmentÃ©, cf la Taverne)
    * Bdd mindset versus bdd outil
    * snapshot/golden-master/approval
      * normatif versus descriptif
      * Approval Testing == Snapshot Testing == Golden Master ?
    * en prod : canary release, alpha testing, beta testing, blue-green, ...
      * Field testing (schneider)
      * user acceptance
    * security testing
      * exemple : [ZAP Proxy](https://www.zaproxy.org/), scanners
    * test de perf
      * rendu accessible par de l'outillage, mais reste rare et hyper-spÃ©cifique en terme de scÃ©nario
      * typologie selon https://grafana.com/load-testing/types-of-load-testing/ : smoke, average load, stress, soak, breakpoint, spike, ...
    * full simulation (Ã  la Matrix)
      * [What's the big deal about Deterministic Simulation Testing?](https://notes.eatonphil.com/2024-08-20-deterministic-simulation-testing.html)
      * [Pierre Zemb : Et si on faisait du simulation-driven development ?](https://www.youtube.com/watch?v=12LO_l90vDk)
    * contract testing
      * Test d'interface d'une third-party (comportement fidÃ¨le aux attentes, rupture d'API, ...), un d'un composant interne Ã  un autre
    * property-based testing + fuzzing
      * property vs fuzzing (cf [article sur la diffÃ©rence](https://www.tedinski.com/2018/12/11/fuzzing-and-property-testing.html))
      * oracle parfois difficile Ã  obtenir, parfois Ã©vident
      * monkey testing (des inputs au hasard, le test n'est pas structurÃ©, aucun scÃ©nario)
    * tests de maintenabilitÃ© ISO 25010 (= modularitÃ© + rÃ©utilisabilitÃ© + analysabilitÃ© + modifiabilitÃ© + testabilitÃ©), cf https://latavernedutesteur.fr/2018/11/19/types-de-tests-iso-25-010-les-tests-de-maintenabilite-7-8/
    * test d'accessibilitÃ©
    * Formal methods, preuves
    * [ISO 25010](https://iso25000.com/images/figures/iso_25010_en.png) et [ISO 25019 orientÃ© usage](https://latavernedutesteur.fr/wp-content/uploads/2023/07/image-1-1024x217.png)
    * Black box / white /glass
    * tests d'architecture (Java = ArchUnit, Python = PyTestArch)
    * test data management
    * tests "statiques" (versus dynamiques) :Linter, typechecker, SonarQube, ... (compilation)
      * des tests qu'il n'y a pas besoin d'Ã©crire, et qui peuvent s'exÃ©cuter sans exÃ©cuter le code (statiques)
      * Rust, tooling
    * Test d'Ã©chafaudage (scaffolding)
      * on les met le temps des travaux, puis on les enlÃ¨ve
    * test hybride : test auto avec vÃ©rif humaine, ou test manuel avec assistance autom
    * test des logs/metrics (cf [mon post LinkedIn](https://www.linkedin.com/posts/julien-lenormand_est-ce-quil-faut-tester-les-logs-je-suis-activity-7285926322604752896-SC6z))
    * London "Mockist"/"Behaviorist" versus Detroit "Classicist"
      * exemple dans un post Linkedin : https://www.linkedin.com/posts/francois-baveye_met-tes-tests-unitaires-%C3%A0-la-poubelle-activity-7370443832401747968-Uc1O
      * tests unitaires : sociables vs solitaires (est-ce que les objets testÃ©s ont leurs dÃ©pendances rÃ©elles ou mockÃ©es), from "Working Effectively with Unit Tests" de Jay Fields
  * techniques et outils pour tester
    * IA
      * rÃ´le de l'IA dans les tests ? (cf [Tao blue/red team](https://mathstodon.xyz/@tao/114915604830689046))
    * advanced features of pytest (or your framework), know your tools
      * fixture
      * mock
      * plugins
    * mock et doublures : mock fake stub spy
      * Cf Martin Fowler (en ref Ã  la fin)
    * chaos testing : chaos monkey + chaos engineering, cf Netflix + [Chaos Monkey Army](https://github.com/Netflix/SimianArmy/wiki/The-Chaos-Monkey-Army)
    * fake time (freezegun en Python, libfaketime + LD_PRELOAD), Reactive instead of passive polling ou sleeping
      * exemple : tester du code qui doit s'exÃ©cuter pendant des mois (harness de test d'endurance)
    * mesure de la couverture de test
      * Code coverage : line/branch/cond
      * 80 ? 90 ? 99 ?
      * Ne suffit pas !!!
      * Cf "1 / 0" avec 100% de coverage.
      * Ou "foo.update()" si foo est null
      * Il y a des branches invisibles (exceptions, donnÃ©es mal modelÃ©es)
      * test guidÃ© par la couverture (sans mention de pourcentage), pour orienter les tests
    * table testing
    * recording/replaying (VCR)
    * mother object, method factories for test objects, data builders, ... https://martinfowler.com/bliki/ObjectMother.html
    * parallÃ©lisation de tests
    * mutation testing
      * du code de prod, et du code de test, pour mesurer la sensibilitÃ© et spÃ©cificitÃ© des tests Ã  la base de code
    * boundary analysis et extreme values
    * rÃ¨gle du : "0, 1, 2 (many), 99999 (too many), error/exception"
    * Page object model (POM)
    * Pairwise pour la couverture, en contrant l'explosion combinatoire (produit cartÃ©sien des paramÃ¨tres)
    * historisation (visuelle) des rÃ©sultats, pour repÃ©rer les tendances, les patterns
    * Technique de refactoring du sandwich (snowcamp) : push IO to the edge (functional core, imperative shell)
    * systrace/ptrace pour interception et fake des appels systÃ¨mes (cf libfaketime pour exemple)
    * HTTPS Man-in-the-Middle (MITM proxy par exemple) plutÃ´t que `ssl_verify=False`
    * trucage DNS via `/etc/hosts` ou `/etc/resolv.conf`
    * anonymiser des donnÃ©es (de prod)
    * risk-based testing : dÃ©termination de quels tests exÃ©cuter en fonction de la criticitÃ© de la fonctionnalitÃ© couverte
    * test impact analysis : dÃ©termination de quels tests exÃ©cuter en fonction de quel code a Ã©tÃ© modifiÃ©
  * techniques de design
    * TDD (cf on peut pas oublier de les faire Ã  la fin si on les fait au dÃ©but), diffÃ©rents sens du mot TDD, ...
      * cf slides ABC + Discovery Day
      * Mon tddd : testable design
      * "Tdd malgrÃ¨ son nom n'est pas une technique de test mais de design"
    * [sans-io](https://sans-io.readthedocs.io/)
    * architecture hÃ©xagonale / clean / onion / ...
    * programmation fonctionnelle
    * profile your tests ! (Ã©viter les "slips/sleeps sales") cf snakeviz marche aussi pour les tests (cf article de Xavier et son setup de DB), tests en parallÃ¨le (cf article du blog de PyPi), Ãªtre rÃ©actif plutÃ´t que passif (cf MQTT tester de Schneider)
      * surveiller la performance des tests autos, ne correspond pas aux tests de performance
    * trunk-based development + feature flags
    * dependency inversion (D de SOLID), SRP
    * inclure des fonctionnalitÃ©s requises par les tests dans le code de prod ? non-prÃ©fÃ©rable mais acceptable
  * philosophie et process
    * Accelerate
      * https://dora.dev/capabilities/ : **test automation + test data management**, but also indirectly code maintainability, documentation quality, job satisfaction, continuous delivery, streamlining change approval, trunk-based development, working in small batches, continuous integration
    * DevOps (cf slide)
    * Domain-Driven Development (DDD)
    * [ISO/IEC 25010:2023](https://www.iso.org/fr/standard/78176.html)
    * gestion des erreurs souvent peu poussÃ©e, manque de contexte dans les logs
      * error model
    * "Clean Test = 3 things : readability, readability, readability" cf Martin Fowler
      * Evolving/surfacing a "testing language" to reveal intent
        * = POM ?

---

# 4. Cas pratique

Notes:
* TODO : 3 exemples de test
  * fonction pure (mais avec de la complexitÃ© interne), quelques cas d'erreur prÃ©vus -> tests fonc, table, edge cases, fuzz, property-based
  * API WEB : mock, contrat, VCR, fake d'API
  * gros Postgres legacy, nouvelle feature

---

# 5. Conclusion

Notes:
* expertise indispensable, il faut s'y mettre, dans un environnement semi-hostile (vocab, Ã©quipe, rythme, outillage, ...) -> CI, run local. C'est une partie de l'ingÃ©nierie

---

# 6. Nos recommandations

* TODO @jonathan d'autres Ã  rajouter ?
* TODO @julien sort par catÃ©gories + multi-pages
* [Jeremy Sorent - J'Ã©cris de tests sans pleurer maintenant](https://www.youtube.com/watch?v=2S9TxoTE8BA) : TODO @julien mon avis
* [Michael feathers - Working effectively with legacy code](https://softwareengineering.stackexchange.com/questions/122014/what-are-the-key-points-of-working-effectively-with-legacy-code) : spoiler Ã§a parle Ã©normÃ©ment de test !
* [Dwayne Richard Hipp - How SQLite Is Tested](https://www.sqlite.org/testing.html) : un exemple de comment n'avoir quasi aucun bug pour un des logiciels les plus utilisÃ© au monde
* [Adam Bender - SMURF: Beyond the Test Pyramid](https://testing.googleblog.com/2024/10/smurf-beyond-test-pyramid.html) : un exemple par Google de dÃ©tricoter la pyramide des tests dans une vision complÃ©mentaire des tests selon leurs propriÃ©tÃ©s techniques
* [MiÅ¡ko Hevery - Writing Testable Code](https://testing.googleblog.com/2008/08/by-miko-hevery-so-you-decided-to.html) : un ensemble de conseils pour rendre son code testable, dont le premier point ("Mixing object graph construction with application logic") est trop mÃ©connu
* [BiteCode - Testing with Python (part 4): why and what to test?](https://www.bitecode.dev/p/testing-with-python-part-4-why-and) : toute la sÃ©rie d'articles vaut le dÃ©tour, mais cet Ã©pisode s'attarde sur, sans le nommer ainsi, la stratÃ©gie de test
<<<<<<< HEAD
* [J.B. Rainsberger - Integrated Tests Are A Scam](https://www.youtube.com/watch?v=fhFa4tkFUFw) : une vision trÃ¨s centrÃ©e sur les tests de contrat, pour pousser les "tests d'intÃ©gration" Ã  ne porter que sur l'anneau externe de l'application, en interaction avec son environnement (runtime, dÃ©pendances externes, ...), tout le reste est couvert par du test "unitaire" de contrat + des mocks de collaborateurs
* [Gary Bernhardt - Boundaries](https://www.destroyallsoftware.com/talks/boundaries) : comment dÃ©couper son application pour faciliter sa testabilitÃ© (notion de "context domain" du DDD)
* [Brandon Rhodes - The Clean Architecture in Python](https://www.youtube.com/watch?v=DJtef410XaM) : Ã  quels problÃ¨mes elle rÃ©pond et comment la mettre en place

Notes:
  * trop de recommandations, faut en garder que 3, 4, ou 5
=======
* et + en annexe !
>>>>>>> 89d9343 (apply review comments)

---

# Bibliographie (et quantitÃ© pertinente)

* Titus Winters, Tom Manschrek et Hyrum Wright - Software Engineering at Google ([online](https://abseil.io/resources/swe-book))
* TODO @Julien Software Craft (Dunod)
* TODO @Julien Clean Code 2008
* TODO @Julien Software Architecture in practice
* TODO @Julien A philosophy of software design
* TODO @Julien Working effectively with legacy code
  * (recommandÃ© auparavant)
* TODO @Julien Test-Driven Development By Example
* TODO @Julien ItÃ©rations Product(ives)
* TODO @Julien Pourquoi votre stratÃ©gie de tests end-to-end Ã©choue ?
* Refactoring, Martin Fowler, 2Ã¨me Ã©dition franÃ§aise, 2019 (410 pages)
  * Chapitre 4 "CrÃ©ation des tests" (15 pages)

Manquants :
* TODO @Julien Agile Testing - Lisa Crispin et Janet Gregory
* Growing Object-Oriented Software Guided by Tests - ... TODO @julien
* Unit Testing - Baptiste Lyet TODO @julien
* How to test legacy code - emily bache TODO @julien
* Can we test it? Yes we can - Mitchell Hashimoto TODO @julien

---

# CrÃ©dits photos

Notes:
* [mÃ¨me de source inconnue, sur yaplakal.com](https://s00.yaplakal.com/pics/pics_preview/4/4/7/10845744.jpg)
* [image de crÃ¨te, sur Wikimedia](https://commons.wikimedia.org/wiki/File:Starorobocia%C5%84ski_Wierch_a3.jpg)
* TODO later

---

# Remerciements

Notes:
* Damien Roulier, Eric Papazian, Mathieu Mattringe, Rachel Da Silva, Francky Flamant, Fanny Velsin, Victor Lambret

---

# Questions

Notes:
* TODO image
* TODO lien vers les slides : https://github.com/Lenormju/talk-enfer-test-autos/

---

# Plus de recommandations !!

* [Joel "on Software" Spolsky - Hard-assed Bug Fixinâ€™](https://www.joelonsoftware.com/2001/07/31/hard-assed-bug-fixin/) : est-ce que tous les bugs devraient Ãªtre corrigÃ©s ? Ã§a dÃ©pend.
* [Mathieu Eveillard - 50 shades of tests](https://www.mathieueveillard.com/blog/50-shades-of-tests) : des dÃ©finitions plutÃ´t claires pour diffÃ©rents types de test, leur positionnement sur 3 dimensions, au-delÃ  de la pyramide de tests
* [Marc Hage Chahine (La Taverne du Testeur) - Que doit-on attendre dâ€™un testeur ?](https://latavernedutesteur.fr/2025/09/15/que-doit-on-attendre-dun-testeur/) : les diffÃ©rentes dimensions du mÃ©tier de testeur
* [Arnaud Lemaire - From code to consequences](https://www.youtube.com/watch?v=muRdH9u7gO4) : en quoi les "full cycle engineers" sont importants pour mener Ã  bien des projets
* [Colin Damon - Ma typologie de tests et leur Ã©quilibrage](https://www.linkedin.com/posts/colin-damon_mettre-en-place-une-strat%C3%A9gie-de-tests-qui-activity-7343525861444247552-6BJY) : un exemple de "pyramide" dans un contexte prÃ©cis
* [Redowan Delowar - Test state, not interactions](http://rednafi.com/go/test-state-not-interactions/) : pourquoi les tests proposÃ©s par des LLMs ne sont pas nÃ©cessairement les bons, et comment faire mieux (par exemple privilÃ©gier les fakes aux mocks)
* [Jeff Atwood (CodingHorror) - Falling Into The Pit of Success](https://blog.codinghorror.com/falling-into-the-pit-of-success/) : comment ne plus avoir besoin de se battre pour que la qualitÃ© ne dÃ©gringole pas ?
* [Antoine Mazure - Tests pragmatiques : comment presque arreÌ‚ter les tests automatiseÌs ?](https://www.youtube.com/watch?app=desktop&v=ohV6GvCIeLY) : un exemple de tester la mauvaise chose, et de comment mieux tester avec pourtant moins de tests
* [Jules Poissonnet et Antoine Caron - Tester c'est tricher)](https://www.youtube.com/watch?v=I_zNxGqRI3w) : une vision d'ensemble, claire et illustrÃ©e, de la dÃ©marche de test, du vocabulaire et des difficultÃ©s
* [Christophe BrÃ©heret-Girardin - Comment une architecture influence votre stratÃ©gie de test ?](https://m.youtube.com/watch?v=IeOa6XWxkxg)
* [Ham Vocke - The Practical Test Pyramid](https://martinfowler.com/articles/practical-test-pyramid.html) : des exemples concrets dans un contexte clair de diffÃ©rents types de test, et des limitations de la pyramide de Mike Cohn
* [Martin Fowler - Mocks Aren't Stubs](https://martinfowler.com/articles/mocksArentStubs.html) : dÃ©finitions claires de tous les "*test doubles*" (dummy, stub, fake, spy, mock) par Martin Fowler
  * [Martin Fowler - Test Double](https://martinfowler.com/bliki/TestDouble.html) : en version ultra-abrÃ©gÃ©e
* [AnaÃ«l Lefebvre - Comment en finir avec la fragilitÃ© des tests unitaires](https://www.sqli.com/fr-fr/insights/comment-en-finir-avec-la-fragilite-des-tests-unitaires) : un contexte clair, une explication de FIRST, et une mÃ©thodo ("ZOMBIES") pour identifier les cas de test
* [The Grug Brained Developer - Testing](https://grugbrain.dev/#grug-on-testing) : des conseils de programmation pertinents, mais rÃ©digÃ©s par "Grug" qui a une capacitÃ© limitÃ©e, et qui le revendique (!)
* [Qalisty et AnaÃ¯s Fournier - Comment sâ€™en sortir lorsquâ€™on est 1 testeuse face Ã  25 dÃ©veloppeurs ?](https://open.spotify.com/episode/1nwA9nLdezVk6mzWu39T7a) : des techniques concrÃ¨tes pour mettre en place une culture qualitÃ© et une stratÃ©gie
* [Victor Lambret - Le TDD sans commencer par les tests](https://www.youtube.com/watch?v=Ddarw3wUXQY) : TODO @julien mon avis
* [Mike Wacker - Just Say No to More End-to-End Tests](https://testing.googleblog.com/2015/04/just-say-no-to-more-end-to-end-tests.html) : les tests unitaires seraient ceux qui compte le plus, du point de vue des utilisateurs (!)
* [Simon Stewart - Test Sizes](https://testing.googleblog.com/2010/12/test-sizes.html) : caractÃ©risation des tests (en un tableau), non pas en unitaires versus end-to-end, mais en small versus big, en fonction de leurs IO
* [Igor RoztropiÅ„ski - Unit, Integration, E2E, Contract, X tests: what should we focus on?](https://binaryigor.com/unit-integration-e2e-contract-x-tests-what-should-we-focus-on.html) : de l'intÃ©rÃªt de favoriser les tests d'intÃ©gration ("in-between", n'Ã©tant pas extrÃªmes), et les tests de contrat
* [Kent C. Dodds - Write tests. Not too many. Mostly integration.](https://kentcdodds.com/blog/write-tests) : introduit la pyramide "trophy" (en incluant les tests statiques) pour des applis JS, avec surtout des tests d'intÃ©gration
* [Seb Rose - Making a meal of architectural alignment and the test-induced-design-damage fallacy](https://claysnow.co.uk/architectural-alignment-and-test-induced-design-damage-fallacy/) : une bonne leÃ§on d'Ã©quilibre et de pragmatisme
* [IFTTD #43.src - Test: Tester c'est douter avec Arnaud Lemaire](https://open.spotify.com/episode/2gRex0ajRA1oVc7DZBL0B9) : TODO @julien
* [CÃ©cilia Bossard et Angi Guyard - On nâ€™aurait pas oubliÃ© un truc dans le craft !?](https://www.youtube.com/watch?v=yVmKkRH60VI) : spoiler il s'agit des tests utilisateurs

---

# Exemple de citation trompeuse

> "I get paid for code that works, not for tests [...]"

- Kent Beck

[...]

> I get paid for code that works, not for tests, so my philosophy is to test as little as possible to reach a given level of confidence (I suspect this level of confidence is high compared to industry standards, but that could just be hubris).
> If I don't typically make a kind of mistake (like setting the wrong variables in a constructor), I don't test for it
> I do tend to make sense of test errors, so I'm extra careful when I have logic with complicated conditionals.
> When coding on a team, I modify my strategy to carefully test code that we, collectively, tend to get wrong.

Source : [Stack Overflow en 2008 : "How deep are your unit tests?"](https://stackoverflow.com/a/153565/11384184)

---

# Abstract

L'enfer des tests auto

Des tests automatiques, on sait qu'il faut en faire. Mais trop souvent c'est une vÃ©ritable corvÃ©e : c'est lent, pas fiable, compliquÃ© Ã  lancer, compliquÃ© Ã  maintenir, voire compliquÃ© Ã  Ã©crire tout court. Pourquoi c'est si dur ? Et comment reprendre le contrÃ´le ?

Il va falloir reprendre depuis le dÃ©but : pourquoi on teste ? et on teste quoi ? et on teste comment ?
Ensuite, il va falloir revoir un peu les bases du test : un brin de vocabulaire, et surtout de la mÃ©thodologie (dÃ©pendances, interfaces, contrat, mock, simulateur, fixture, inversion, ...).
On fera un petit dÃ©tour par l'architecture, parce que la testabilitÃ© est une propriÃ©tÃ© Ã  prendre en compte dÃ¨s le design, ou bien il faudra jouer du scalpel ou du pied-de-biche par la suite pour les faire rentrer.
Enfin, on passera Ã  l'implÃ©mentation, et avec quelques bonnes pratiques et le bon outillage (framework de test, TestContainers, CI, ...), Ã§a se passera plutÃ´t bien.

A travers des cas concrets tirÃ©s de nos expÃ©riences, suivez-nous sur le chemin pour quitter l'enfer des tests.
